{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can use to construct your information needs (topics).\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). Throughout the course, you will try to improve upon this baseline retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/) for loading the retrieval index and the [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset: the union of the IR Anthology and the ACL Anthology\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/anthology-20240408-training')\n",
    "\n",
    "# A PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval system that first retrieves the top-10 results and then adds the text of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarative pipeline:\n",
    "# Step 1: retrieve the top 10 results with BM25\n",
    "# Step 2: Add the document text\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\") %10 >> pt.text.get_text(pt_dataset, \"text\")\n",
    "\n",
    "# do not truncate text in the dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Do Some Searches to Refine your Topic\n",
    "\n",
    "You can search via `bm25.search(\"your query\")`.\n",
    "In the following, we see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94720</td>\n",
       "      <td>2009.cikm_conference-2009.247</td>\n",
       "      <td>0</td>\n",
       "      <td>30.617689</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>A machine learning approach for improved BM25 retrieval\\n\\n\\n ABSTRACTDespite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations. We determine the effectiveness of BM25 on various document fields. We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes. We also find query click information to be the single most important field for retrieval. In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25. Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures. We demonstrate the advantages of our model on a very large real-world Web data collection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>94817</td>\n",
       "      <td>2004.cikm_conference-2004.6</td>\n",
       "      <td>1</td>\n",
       "      <td>25.313076</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>Simple BM25 extension to multiple weighted fields\\n\\n\\n ABSTRACTThis paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the nonlinear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>73672</td>\n",
       "      <td>2020.sigirconf_workshop-2020birds.11</td>\n",
       "      <td>2</td>\n",
       "      <td>21.780742</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>BM25-FIC: Information Content-based Field Weighting for BM25F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>80315</td>\n",
       "      <td>2012.sigirconf_conference-2012.94</td>\n",
       "      <td>3</td>\n",
       "      <td>21.264897</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>Extending BM25 with multiple query operators\\n\\n\\n ABSTRACTTraditional probabilistic relevance frameworks for informational retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the successful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collections, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effectiveness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>74194</td>\n",
       "      <td>2010.ntcir_workshop-2010.54</td>\n",
       "      <td>4</td>\n",
       "      <td>18.603729</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>A KNN Research Paper Classification Method Based on Shared Nearest Neighbor\\n\\n\\n The patents cover almost all the latest, the most active innovative technical information in technical fields, therefore patent classification has great application value in the patent research domain. This paper presents a KNN text categorization method based on shared nearest neighbor, effectively combining the BM25 similarity calculation method and the Neighborhood Information of samples. The effectiveness of this method has been fully verified in the NTCIR-8 Patent Classification evaluation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>110592</td>\n",
       "      <td>2018.trec_conference-2018.44</td>\n",
       "      <td>5</td>\n",
       "      <td>16.752104</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>The University of Padua IMS Research Group at TREC 2018 Precision Medicine Track\\n\\n\\n We report on the participation of the Information Management System (IMS) Research Group of the University of Padua in the second task of the Precision Medicine Track at TREC 2018: the Clinical Trials task. We designed a procedure to: i) expand query terms iteratively, based on knowledge bases, to increase the probability of finding relevant trials by adding neoplasm, gene, and protein term variants to the initial query; ii) filter out trials based on demographic data. We submitted three runs: a plain BM25 using the provided textual fields &lt;gene&gt; and &lt;disease&gt; as query, a BM25 with a first knowledge-based query expansion, and another BM25 with an additional knowledge-based query expansion. This initial set of experiments lays the ground for a deeper study on the effectiveness of (automatic) knowledge-based expansion techniques in the context of precision medicine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>76458</td>\n",
       "      <td>2007.clef_workshop-2007w.33</td>\n",
       "      <td>6</td>\n",
       "      <td>16.250935</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>Dublin City University at CLEF 2007: Cross Language Speech Retrieval (CL-SR) Experiments\\n\\n\\n The Dublin City University participated in the CLEF 2007 CL-SR English task. For CLEF 2007 we concentrated primarily on the issues of topic translation, combining this with search field combination and pseudo relevance feedback methods used for our CLEF 2006 submissions. Topics were translated into English using the Yahoo! BabelFish free online translation service combined with domain-specific translation lexicons gathered automatically from Wikipedia. We explored alternative translations methods with document retrieval based the combination of the multiple document fields using the BM25F field combination model. Our results indicate that extending machine translation tools using automatically generated domain-specific translation dictionaries can provide improved CLIR effectiveness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>126358</td>\n",
       "      <td>2013.tois_journal-ir0anthology0volumeA31A3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>16.120809</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>About learning models with multiple query-dependent features\\n\\n\\n Several questions remain unanswered by the existing literature concerning the deployment of querydependent features within learning to rank. In this work, we investigate three research questions in order to empirically ascertain best practices for learning-to-rank deployments. (i) Previous work in data fusion that pre-dates learning to rank showed that while different retrieval systems could be effectively combined, the combination of multiple models within the same system was not as effective. In contrast, the existing learning-to-rank datasets (e.g., LETOR), often deploy multiple weighting models as query-dependent features within a single system, raising the question as to whether such a combination is needed. (ii) Next, we investigate whether the training of weighting model parameters, traditionally required for effective retrieval, is necessary within a learning-to-rank context. (iii) Finally, we note that existing learning-to-rank datasets use weighting model features calculated on different fields (e.g., title, content, or anchor text), even though such weighting models have been criticized in the literature. Experiments addressing these three questions are conducted on Web search datasets, using various weighting models as query-dependent and typical query-independent features, which are combined using three learning-to-rank techniques. In particular, we show and explain why multiple weighting models should be deployed as features. Moreover, we unexpectedly find that training the weighting model's parameters degrades learned model's effectiveness. Finally, we show that computing a weighting model separately for each field is less effective than more theoretically-sound field-based weighting models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>101702</td>\n",
       "      <td>2018.ictir_conference-2018.36</td>\n",
       "      <td>8</td>\n",
       "      <td>15.768577</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>StatBM25: An Aggregative and Statistical Approach for Document Ranking\\n\\n\\n ABSTRACTIn Information Retrieval and Web Search, BM25 is one of the most influential probabilistic retrieval formulas for document weighting and ranking. BM25 involves three parameters k 1 , k 3 and b, which provide scalar approximation and scaling of important document features such as term frequency, document frequency, and document length. We investigate in this paper aggregative and statistical document features for document ranking. Shortly speaking, a statistically adjusted BM25 is used to score in an aggregative way on virtual documents, which are generated by randomly combining documents from the original collection. The problem size, in the number of virtual documents to be ranked, is an expansion to the problem size of the original problem. As a result, ranking is actually realized through performing statistical sampling. Rejection Sampling, a simple Monte Carlo sampling method is used at present. This new framework is called StatBM25, in emphasizing first the fact that the original problem domain space is K-expanded (a concept to be further explained in the paper); Further, statistical sampling is employed in the model. Empirical studies are performed on several standard test collections, where StatBM25 demonstrates convincingly high degree of both uniqueness and effectiveness compared to BM25. This means, in our belief, that StatBM25 as a statistically smoothed and normalized variant to BM25, might eventually lead to discoveries of useful new statistic measures for document ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>109892</td>\n",
       "      <td>2019.trec_conference-2019.2</td>\n",
       "      <td>9</td>\n",
       "      <td>15.583703</td>\n",
       "      <td>how to combine bm25 for multiple fields</td>\n",
       "      <td>An Evaluation of Weakly-Supervised DeepCT in the TREC 2019 Deep Learning Track\\n\\n\\n This paper describes our participation in the TREC 2019 Deep Learning Track document ranking task. We developed a deep learning based document term weighting approach based on our previous work of DeepCT. It used the contextualized token embeddings generated by BERT to estimate a term's importance in passages, and combines passage term weights into document-level term weights. The weighted document is stored in an ordinary inverted index and searched using a multi-field BM25, which is efficient. We tested two ways of training DeepCT: a query-based method using sparse relevant query-document pairs, and a weaklysupervised method using document title-body pairs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                                         docno  rank      score  \\\n",
       "0   1   94720                 2009.cikm_conference-2009.247     0  30.617689   \n",
       "1   1   94817                   2004.cikm_conference-2004.6     1  25.313076   \n",
       "2   1   73672          2020.sigirconf_workshop-2020birds.11     2  21.780742   \n",
       "3   1   80315             2012.sigirconf_conference-2012.94     3  21.264897   \n",
       "4   1   74194                   2010.ntcir_workshop-2010.54     4  18.603729   \n",
       "5   1  110592                  2018.trec_conference-2018.44     5  16.752104   \n",
       "6   1   76458                   2007.clef_workshop-2007w.33     6  16.250935   \n",
       "7   1  126358  2013.tois_journal-ir0anthology0volumeA31A3.0     7  16.120809   \n",
       "8   1  101702                 2018.ictir_conference-2018.36     8  15.768577   \n",
       "9   1  109892                   2019.trec_conference-2019.2     9  15.583703   \n",
       "\n",
       "                                     query  \\\n",
       "0  how to combine bm25 for multiple fields   \n",
       "1  how to combine bm25 for multiple fields   \n",
       "2  how to combine bm25 for multiple fields   \n",
       "3  how to combine bm25 for multiple fields   \n",
       "4  how to combine bm25 for multiple fields   \n",
       "5  how to combine bm25 for multiple fields   \n",
       "6  how to combine bm25 for multiple fields   \n",
       "7  how to combine bm25 for multiple fields   \n",
       "8  how to combine bm25 for multiple fields   \n",
       "9  how to combine bm25 for multiple fields   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            A machine learning approach for improved BM25 retrieval\\n\\n\\n ABSTRACTDespite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations. We determine the effectiveness of BM25 on various document fields. We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes. We also find query click information to be the single most important field for retrieval. In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25. Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures. We demonstrate the advantages of our model on a very large real-world Web data collection.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Simple BM25 extension to multiple weighted fields\\n\\n\\n ABSTRACTThis paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the nonlinear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               BM25-FIC: Information Content-based Field Weighting for BM25F  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Extending BM25 with multiple query operators\\n\\n\\n ABSTRACTTraditional probabilistic relevance frameworks for informational retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the successful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collections, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effectiveness.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A KNN Research Paper Classification Method Based on Shared Nearest Neighbor\\n\\n\\n The patents cover almost all the latest, the most active innovative technical information in technical fields, therefore patent classification has great application value in the patent research domain. This paper presents a KNN text categorization method based on shared nearest neighbor, effectively combining the BM25 similarity calculation method and the Neighborhood Information of samples. The effectiveness of this method has been fully verified in the NTCIR-8 Patent Classification evaluation.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The University of Padua IMS Research Group at TREC 2018 Precision Medicine Track\\n\\n\\n We report on the participation of the Information Management System (IMS) Research Group of the University of Padua in the second task of the Precision Medicine Track at TREC 2018: the Clinical Trials task. We designed a procedure to: i) expand query terms iteratively, based on knowledge bases, to increase the probability of finding relevant trials by adding neoplasm, gene, and protein term variants to the initial query; ii) filter out trials based on demographic data. We submitted three runs: a plain BM25 using the provided textual fields <gene> and <disease> as query, a BM25 with a first knowledge-based query expansion, and another BM25 with an additional knowledge-based query expansion. This initial set of experiments lays the ground for a deeper study on the effectiveness of (automatic) knowledge-based expansion techniques in the context of precision medicine.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Dublin City University at CLEF 2007: Cross Language Speech Retrieval (CL-SR) Experiments\\n\\n\\n The Dublin City University participated in the CLEF 2007 CL-SR English task. For CLEF 2007 we concentrated primarily on the issues of topic translation, combining this with search field combination and pseudo relevance feedback methods used for our CLEF 2006 submissions. Topics were translated into English using the Yahoo! BabelFish free online translation service combined with domain-specific translation lexicons gathered automatically from Wikipedia. We explored alternative translations methods with document retrieval based the combination of the multiple document fields using the BM25F field combination model. Our results indicate that extending machine translation tools using automatically generated domain-specific translation dictionaries can provide improved CLIR effectiveness.  \n",
       "7  About learning models with multiple query-dependent features\\n\\n\\n Several questions remain unanswered by the existing literature concerning the deployment of querydependent features within learning to rank. In this work, we investigate three research questions in order to empirically ascertain best practices for learning-to-rank deployments. (i) Previous work in data fusion that pre-dates learning to rank showed that while different retrieval systems could be effectively combined, the combination of multiple models within the same system was not as effective. In contrast, the existing learning-to-rank datasets (e.g., LETOR), often deploy multiple weighting models as query-dependent features within a single system, raising the question as to whether such a combination is needed. (ii) Next, we investigate whether the training of weighting model parameters, traditionally required for effective retrieval, is necessary within a learning-to-rank context. (iii) Finally, we note that existing learning-to-rank datasets use weighting model features calculated on different fields (e.g., title, content, or anchor text), even though such weighting models have been criticized in the literature. Experiments addressing these three questions are conducted on Web search datasets, using various weighting models as query-dependent and typical query-independent features, which are combined using three learning-to-rank techniques. In particular, we show and explain why multiple weighting models should be deployed as features. Moreover, we unexpectedly find that training the weighting model's parameters degrades learned model's effectiveness. Finally, we show that computing a weighting model separately for each field is less effective than more theoretically-sound field-based weighting models.  \n",
       "8                                                                                                                                                                                                                StatBM25: An Aggregative and Statistical Approach for Document Ranking\\n\\n\\n ABSTRACTIn Information Retrieval and Web Search, BM25 is one of the most influential probabilistic retrieval formulas for document weighting and ranking. BM25 involves three parameters k 1 , k 3 and b, which provide scalar approximation and scaling of important document features such as term frequency, document frequency, and document length. We investigate in this paper aggregative and statistical document features for document ranking. Shortly speaking, a statistically adjusted BM25 is used to score in an aggregative way on virtual documents, which are generated by randomly combining documents from the original collection. The problem size, in the number of virtual documents to be ranked, is an expansion to the problem size of the original problem. As a result, ranking is actually realized through performing statistical sampling. Rejection Sampling, a simple Monte Carlo sampling method is used at present. This new framework is called StatBM25, in emphasizing first the fact that the original problem domain space is K-expanded (a concept to be further explained in the paper); Further, statistical sampling is employed in the model. Empirical studies are performed on several standard test collections, where StatBM25 demonstrates convincingly high degree of both uniqueness and effectiveness compared to BM25. This means, in our belief, that StatBM25 as a statistically smoothed and normalized variant to BM25, might eventually lead to discoveries of useful new statistic measures for document ranking.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            An Evaluation of Weakly-Supervised DeepCT in the TREC 2019 Deep Learning Track\\n\\n\\n This paper describes our participation in the TREC 2019 Deep Learning Track document ranking task. We developed a deep learning based document term weighting approach based on our previous work of DeepCT. It used the contextualized token embeddings generated by BERT to estimate a term's importance in passages, and combines passage term weights into document-level term weights. The weighted document is stored in an ordinary inverted index and searched using a multi-field BM25, which is efficient. We tested two ways of training DeepCT: a query-based method using sparse relevant query-document pairs, and a weaklysupervised method using document title-body pairs.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search('how to combine bm25 for multiple fields')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114365</td>\n",
       "      <td>2011.tweb_journal-ir0anthology0volumeA5A4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.264262</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Efficient Search Engine Measurements\\n\\n\\n We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities  are biased due to inaccurate approximation of the so called \"document degrees\". In addition, the estimators in  are quite costly, due to their reliance on rejection sampling.We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in . Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>96855</td>\n",
       "      <td>2013.cikm_conference-2013.148</td>\n",
       "      <td>1</td>\n",
       "      <td>22.188646</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Maintaining discriminatory power in quantized indexes\\n\\n\\n ABSTRACTThe time cost of searching with an inverted index is directly proportional to the number of postings processed and the cost of processing each posting. Dynamic pruning reduces the number of postings examined. Pre-calculation then quantization of term / document weights reduces the cost of evaluating each posting. The effect of quantization on precision, latency, and index size is examined herein. We show empirically that there is an ideal size (in bits) for storing the quantized scores. Increasing this adversely affects index size and search latency; decreasing it adversely affects precision. We observe a relationship between the collection size and ideal quantization size, and provide a way to determine the number of bits to use from the collection size.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>101928</td>\n",
       "      <td>2007.wwwconf_conference-2007.41</td>\n",
       "      <td>2</td>\n",
       "      <td>22.012013</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Efficient search engine measurements\\n\\n\\n ABSTRACTWe address the problem of measuring global quality metrics of search engines, like corpus size, index freshness, and density of duplicates in the corpus. The recently proposed estimators for such metrics  suffer from significant bias and/or poor performance, due to inaccurate approximation of the so called \"document degrees\".We present two new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.Building on an idea from [6], we discuss Rao Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in significant performance improvements, while not compromising accuracy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>82030</td>\n",
       "      <td>2007.sigirconf_conference-2007.140</td>\n",
       "      <td>3</td>\n",
       "      <td>21.496510</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Estimating collection size with logistic regression\\n\\n\\n ABSTRACTCollection size is an important feature to represent the content summaries of a collection, and plays a vital role in collection selection for distributed search. In uncooperative environments, collection size estimation algorithms are adopted to estimate the sizes of collections with their search interfaces. This paper proposes heterogeneous capture (HC) algorithm, in which the capture probabilities of documents are modeled with logistic regression. With heterogeneous capture probabilities, HC algorithm estimates collection size through conditional maximum likelihood. Experimental results on real web data show that our HC algorithm outperforms both multiple capture-recapture and capture history algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>91071</td>\n",
       "      <td>2012.ecir_conference-2012.63</td>\n",
       "      <td>4</td>\n",
       "      <td>21.463009</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>On the Size of Full Element-Indexes for XML Keyword Search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>85206</td>\n",
       "      <td>2010.riao_conference-2010.15</td>\n",
       "      <td>5</td>\n",
       "      <td>20.910967</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Estimating the indexability of multimedia descriptors for similarity searching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>94303</td>\n",
       "      <td>2008.cikm_conference-2008.71</td>\n",
       "      <td>6</td>\n",
       "      <td>20.274822</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Can phrase indexing help to process non-phrase queries?\\n\\n\\n ABSTRACTModern web search engines, while indexing billions of web pages, are expected to process queries and return results in a very short time. Many approaches have been proposed for efficiently computing top-k query results, but most of them ignore one key factor in the ranking functions of commercial search engines -term-proximity, which is the metric of the distance between query terms in a document. When term-proximity is included in ranking functions, most of the existing top-k algorithms will become inefficient. To address this problem, in this paper we propose to build a compact phrase index to speed up the search process when incorporating the term-proximity factor. The compact phrase index can help more accurately estimate the score upper bounds of unknown documents. The size of the phrase index is controlled by including a small portion of phrases which are possibly helpful for improving search performance. Phrase index has been used to process phrase queries in existing work. It is, however, to the best of our knowledge, the first time that phrase index is used to improve the performance of generic queries. Experimental results show that, compared with the state-of-the-art top-k computation approaches, our approach can reduce average query processing time to 1/5 for typical setttings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>102560</td>\n",
       "      <td>2011.wwwconf_conference-2011.52</td>\n",
       "      <td>7</td>\n",
       "      <td>20.273570</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Inverted index compression via online document routing\\n\\n\\n ABSTRACTModern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>124242</td>\n",
       "      <td>2014.ipm_journal-ir0anthology0volumeA50A6.1</td>\n",
       "      <td>8</td>\n",
       "      <td>20.151820</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Indexing and Self-indexing sequences of IEEE 754 double precision numbers\\n\\n\\n a b s t r a c tSuccinct data structures were designed to store and/or index data with a relatively small alphabet size, a rather skewed distribution and/or, a considerable amount of repetitiveness. Although many of them were developed to handle text, they have been used with other data types, like biological collections or source code. However, there are no applications of succinct data structures in the case of floating point data, the obvious reason is that this data type does not usually fulfill the aforementioned requirements.In this work, we present four solutions to store and index floating point data that take advantage of the latest developments in succinct data structures.The first one is based on the well-known inverted index. It consumes space around the size of the source data, providing appealing search times. The other three solutions are based on self-indexing structures. The first one uses a binary Huffman-shaped wavelet tree. It is never the winner in our experiments, but still yields a good balance between space and search performance. The second one is based on wavelet trees on bytecodes, and obtains the best space/time trade-off in most scenarios. The last one is based on Sadakane's Compressed Suffix Array. It excels in space at the expense of less performance at searches.Including a representation of the original data, our indexes occupy from around 70% to 115% of the size of the original collection, and permit fast indexed searches within it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>101929</td>\n",
       "      <td>2007.wwwconf_conference-2007.42</td>\n",
       "      <td>9</td>\n",
       "      <td>19.810760</td>\n",
       "      <td>how to estimate the size of a proprietary search index</td>\n",
       "      <td>Efficient search in large textual collections with redundancy\\n\\n\\n ABSTRACTCurrent web search engines focus on searching only the most recent snapshot of the web. In some cases, however, it would be desirable to search over collections that include many different crawls and versions of each page. One important example of such a collection is the Internet Archive, though there are many others. Since the data size of such an archive is multiple times that of a single snapshot, this presents us with significant performance challenges. Current engines use various techniques for index compression and optimized query execution, but these techniques do not exploit the significant similarities between different versions of a page, or between different pages.In this paper, we propose a general framework for indexing and query processing of archival collections and, more generally, any collections with a sufficient amount of redundancy. Our approach results in significant reductions in index size and query processing costs on such collections, and it is orthogonal to and can be combined with the existing techniques. It also supports highly efficient updates, both locally and over a network. Within this framework, we describe and evaluate different implementations that trade off index size versus CPU cost and other factors, and discuss applications ranging from archival web search to local search of web sites, email archives, or file systems. We present experimental results based on search engine query log and a large collection consisting of multiple crawls.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                                        docno  rank      score  \\\n",
       "0   1  114365  2011.tweb_journal-ir0anthology0volumeA5A4.1     0  23.264262   \n",
       "1   1   96855                2013.cikm_conference-2013.148     1  22.188646   \n",
       "2   1  101928              2007.wwwconf_conference-2007.41     2  22.012013   \n",
       "3   1   82030           2007.sigirconf_conference-2007.140     3  21.496510   \n",
       "4   1   91071                 2012.ecir_conference-2012.63     4  21.463009   \n",
       "5   1   85206                 2010.riao_conference-2010.15     5  20.910967   \n",
       "6   1   94303                 2008.cikm_conference-2008.71     6  20.274822   \n",
       "7   1  102560              2011.wwwconf_conference-2011.52     7  20.273570   \n",
       "8   1  124242  2014.ipm_journal-ir0anthology0volumeA50A6.1     8  20.151820   \n",
       "9   1  101929              2007.wwwconf_conference-2007.42     9  19.810760   \n",
       "\n",
       "                                                    query  \\\n",
       "0  how to estimate the size of a proprietary search index   \n",
       "1  how to estimate the size of a proprietary search index   \n",
       "2  how to estimate the size of a proprietary search index   \n",
       "3  how to estimate the size of a proprietary search index   \n",
       "4  how to estimate the size of a proprietary search index   \n",
       "5  how to estimate the size of a proprietary search index   \n",
       "6  how to estimate the size of a proprietary search index   \n",
       "7  how to estimate the size of a proprietary search index   \n",
       "8  how to estimate the size of a proprietary search index   \n",
       "9  how to estimate the size of a proprietary search index   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Efficient Search Engine Measurements\\n\\n\\n We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities  are biased due to inaccurate approximation of the so called \"document degrees\". In addition, the estimators in  are quite costly, due to their reliance on rejection sampling.We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in . Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Maintaining discriminatory power in quantized indexes\\n\\n\\n ABSTRACTThe time cost of searching with an inverted index is directly proportional to the number of postings processed and the cost of processing each posting. Dynamic pruning reduces the number of postings examined. Pre-calculation then quantization of term / document weights reduces the cost of evaluating each posting. The effect of quantization on precision, latency, and index size is examined herein. We show empirically that there is an ideal size (in bits) for storing the quantized scores. Increasing this adversely affects index size and search latency; decreasing it adversely affects precision. We observe a relationship between the collection size and ideal quantization size, and provide a way to determine the number of bits to use from the collection size.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Efficient search engine measurements\\n\\n\\n ABSTRACTWe address the problem of measuring global quality metrics of search engines, like corpus size, index freshness, and density of duplicates in the corpus. The recently proposed estimators for such metrics  suffer from significant bias and/or poor performance, due to inaccurate approximation of the so called \"document degrees\".We present two new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.Building on an idea from [6], we discuss Rao Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in significant performance improvements, while not compromising accuracy.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Estimating collection size with logistic regression\\n\\n\\n ABSTRACTCollection size is an important feature to represent the content summaries of a collection, and plays a vital role in collection selection for distributed search. In uncooperative environments, collection size estimation algorithms are adopted to estimate the sizes of collections with their search interfaces. This paper proposes heterogeneous capture (HC) algorithm, in which the capture probabilities of documents are modeled with logistic regression. With heterogeneous capture probabilities, HC algorithm estimates collection size through conditional maximum likelihood. Experimental results on real web data show that our HC algorithm outperforms both multiple capture-recapture and capture history algorithms.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          On the Size of Full Element-Indexes for XML Keyword Search  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Estimating the indexability of multimedia descriptors for similarity searching  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Can phrase indexing help to process non-phrase queries?\\n\\n\\n ABSTRACTModern web search engines, while indexing billions of web pages, are expected to process queries and return results in a very short time. Many approaches have been proposed for efficiently computing top-k query results, but most of them ignore one key factor in the ranking functions of commercial search engines -term-proximity, which is the metric of the distance between query terms in a document. When term-proximity is included in ranking functions, most of the existing top-k algorithms will become inefficient. To address this problem, in this paper we propose to build a compact phrase index to speed up the search process when incorporating the term-proximity factor. The compact phrase index can help more accurately estimate the score upper bounds of unknown documents. The size of the phrase index is controlled by including a small portion of phrases which are possibly helpful for improving search performance. Phrase index has been used to process phrase queries in existing work. It is, however, to the best of our knowledge, the first time that phrase index is used to improve the performance of generic queries. Experimental results show that, compared with the state-of-the-art top-k computation approaches, our approach can reduce average query processing time to 1/5 for typical setttings.  \n",
       "7  Inverted index compression via online document routing\\n\\n\\n ABSTRACTModern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Indexing and Self-indexing sequences of IEEE 754 double precision numbers\\n\\n\\n a b s t r a c tSuccinct data structures were designed to store and/or index data with a relatively small alphabet size, a rather skewed distribution and/or, a considerable amount of repetitiveness. Although many of them were developed to handle text, they have been used with other data types, like biological collections or source code. However, there are no applications of succinct data structures in the case of floating point data, the obvious reason is that this data type does not usually fulfill the aforementioned requirements.In this work, we present four solutions to store and index floating point data that take advantage of the latest developments in succinct data structures.The first one is based on the well-known inverted index. It consumes space around the size of the source data, providing appealing search times. The other three solutions are based on self-indexing structures. The first one uses a binary Huffman-shaped wavelet tree. It is never the winner in our experiments, but still yields a good balance between space and search performance. The second one is based on wavelet trees on bytecodes, and obtains the best space/time trade-off in most scenarios. The last one is based on Sadakane's Compressed Suffix Array. It excels in space at the expense of less performance at searches.Including a representation of the original data, our indexes occupy from around 70% to 115% of the size of the original collection, and permit fast indexed searches within it.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Efficient search in large textual collections with redundancy\\n\\n\\n ABSTRACTCurrent web search engines focus on searching only the most recent snapshot of the web. In some cases, however, it would be desirable to search over collections that include many different crawls and versions of each page. One important example of such a collection is the Internet Archive, though there are many others. Since the data size of such an archive is multiple times that of a single snapshot, this presents us with significant performance challenges. Current engines use various techniques for index compression and optimized query execution, but these techniques do not exploit the significant similarities between different versions of a page, or between different pages.In this paper, we propose a general framework for indexing and query processing of archival collections and, more generally, any collections with a sufficient amount of redundancy. Our approach results in significant reductions in index size and query processing costs on such collections, and it is orthogonal to and can be combined with the existing techniques. It also supports highly efficient updates, both locally and over a network. Within this framework, we describe and evaluate different implementations that trade off index size versus CPU cost and other factors, and discuss applications ranging from archival web search to local search of web sites, email archives, or file systems. We present experimental results based on search engine query log and a large collection consisting of multiple crawls.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search('how to estimate the size of a proprietary search index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>80914</td>\n",
       "      <td>2008.sigirconf_conference-2008.179</td>\n",
       "      <td>0</td>\n",
       "      <td>16.740101</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Local approximation of PageRank and reverse PageRank\\n\\n\\n ABSTRACTWe consider the problem of approximating the PageRank of a target node using only local information provided by a link server. We prove that local approximation of PageRank is feasible if and only if the graph has low in-degree and admits fast PageRank convergence. While natural graphs, such as the web graph, are abundant with high in-degree nodes, making local PageRank approximation too costly, we show that reverse natural graphs tend to have low indegree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally. Finally, we demonstrate the usefulness of Reverse PageRank in five different applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>105982</td>\n",
       "      <td>2004.wwwconf_conference-2004at.161</td>\n",
       "      <td>1</td>\n",
       "      <td>15.963780</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Outlink estimation for pagerank computation under missing data\\n\\n\\n ABSTRACTThe enormity and rapid growth of the web-graph forces quantities such as its pagerank to be computed under missing information consisting of outlinks of pages that have not yet been crawled. This paper examines the role played by the size and distribution of this missing data in determining the accuracy of the computed pagerank, focusing on questions such as (i) the accuracy of pageranks under missing information, (ii) the size at which a crawl process may be aborted while still ensuring reasonable accuracy of pageranks, and (iii) algorithms to estimate pageranks under such missing information. The first couple of questions are addressed on the basis of certain simple bounds relating the expected distance between the true and computed pageranks and the size of the missing data. The third question is explored by devising algorithms to predict the pageranks when full information is not available. A key feature of the \"dangling link estimation\" and \"clustered link estimation\" algorithms proposed is that, they do not need to run the pagerank iteration afresh once the outlinks have been estimated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>108644</td>\n",
       "      <td>2005.wwwconf_conference-2005si.72</td>\n",
       "      <td>2</td>\n",
       "      <td>15.854111</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>On the feasibility of low-rank approximation for personalized PageRank\\n\\n\\n ABSTRACTPersonalized PageRank expresses backlink-based page quality around user-selected pages in a similar way to PageRank over the entire Web. Algorithms for computing personalized PageRank on the fly are either limited to a restricted choice of page selection or believed to behave well only on sparser regions of the Web. In this paper we show the feasibility of computing personalized PageRank by a k &lt; 1000 lowrank approximation of the PageRank transition matrix; by our algorithm we may compute an approximate personalized PageRank by multiplying an n × k, a k × n matrix and the n-dimensional personalization vector. Since low-rank approximations are accurate on dense regions, we hope that our technique will combine well with known algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>91900</td>\n",
       "      <td>2002.ecir_conference-2002.5</td>\n",
       "      <td>3</td>\n",
       "      <td>15.834797</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>An Improved Computation of the PageRank Algorithm\\n\\n\\n Abstract. The Google search site (http://www.google.com) exploits the link structure of the Web to measure the relative importance of Web pages. The ranking method implemented in Google is called PageRank . The sum of all PageRank values should be one. However, we notice that the sum becomes less than one in some cases. We present an improved PageRank algorithm that computes the PageRank values of the Web pages correctly. Our algorithm works out well in any situations, and the sum of all PageRank values is always maintained to be one. We also present implementation issues of the improved algorithm. Experimental evaluation is carried out and the results are also discussed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>126869</td>\n",
       "      <td>2010.tois_journal-ir0anthology0volumeA28A3.1</td>\n",
       "      <td>4</td>\n",
       "      <td>15.834797</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Arnoldi versus GMRES for computing pageRank: A theoretical contribution to google's pageRank problem\\n\\n\\n PageRank is one of the most important ranking techniques used in today's search engines. A recent very interesting research track focuses on exploiting efficient numerical methods to speed up the computation of PageRank, among which the Arnoldi-type algorithm and the GMRES algorithm are competitive candidates. In essence, the former deals with the PageRank problem from an eigenproblem, while the latter from a linear system, point of view. However, there is little known about the relations between the two approaches for PageRank. In this article, we focus on a theoretical and numerical comparison of the two approaches. Numerical experiments illustrate the effectiveness of our theoretical results.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94262</td>\n",
       "      <td>2008.cikm_conference-2008.30</td>\n",
       "      <td>5</td>\n",
       "      <td>15.811399</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Local approximation of pagerank and reverse pagerank\\n\\n\\n ABSTRACTWe consider the problem of approximating the PageRank of a target node using only local information provided by a link server. This problem was originally studied by Chen, Gan, and Suel (CIKM  2004), who presented an algorithm for tackling it. We prove that local approximation of PageRank, even to within modest approximation factors, is infeasible in the worst-case, as it requires probing the link server for Ω(n) nodes, where n is the size of the graph. The difficulty emanates from nodes of high in-degree and/or from slow convergence of the PageRank random walk.We show that when the graph has bounded in-degree and admits fast PageRank convergence, then local PageRank approximation can be done using a small number of queries. Unfortunately, natural graphs, such as the web graph, are abundant with high in-degree nodes, making this algorithm (or any other local approximation algorithm) too costly. On the other hand, reverse natural graphs tend to have low in-degree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally.We demonstrate that Reverse PageRank is useful for several applications, including computation of hub scores for web pages, finding influencers in social networks, obtaining good seeds for crawling, and measurement of semantic relatedness between concepts in a taxonomy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>105935</td>\n",
       "      <td>2004.wwwconf_conference-2004at.114</td>\n",
       "      <td>6</td>\n",
       "      <td>15.660582</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Updating pagerank with iterative aggregation\\n\\n\\n ABSTRACTWe present an algorithm for updating the PageRank vector . Due to the scale of the web, Google only updates its famous PageRank vector on a monthly basis. However, the Web changes much more frequently. Drastically speeding the PageRank computation can lead to fresher, more accurate rankings of the webpages retrieved by search engines. It can also make the goal of real-time personalized rankings within reach. On two small subsets of the web, our algorithm updates PageRank using just 25% and 14%, respectively, of the time required by the original PageRank algorithm. Our algorithm uses iterative aggregation techniques  to focus on the slow-converging states of the Markov chain. The most exciting feature of this algorithm is that it can be joined with other PageRank acceleration methods, such as the dangling node lumpability algorithm [6], quadratic extrapolation , and adaptive PageRank , to realize even greater speedups (potentially a factor of 60 or more speedup when all algorithms are combined).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>125218</td>\n",
       "      <td>2008.ipm_journal-ir0anthology0volumeA44A2.21</td>\n",
       "      <td>7</td>\n",
       "      <td>15.499952</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Bringing PageRank to the citation analysis\\n\\n\\n AbstractThe paper attempts to provide an alternative method for measuring the importance of scientific papers based on the Google's PageRank. The method is a meaningful extension of the common integer counting of citations and is then experimented for bringing PageRank to the citation analysis in a large citation network. It offers a more integrated picture of the publications' influence in a specific field. We firstly calculate the PageRanks of scientific papers. The distributional characteristics and comparison with the traditionally used number of citations are then analyzed in detail. Furthermore, the PageRank is implemented in the evaluation of research influence for several countries in the field of Biochemistry and Molecular Biology during the time period of 2000-2005. Finally, some advantages of bringing PageRank to the citation analysis are concluded.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>108466</td>\n",
       "      <td>2002.wwwconf_conference-2002.50</td>\n",
       "      <td>8</td>\n",
       "      <td>15.480179</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>Topic-sensitive PageRank\\n\\n\\n ABSTRACTIn the original PageRank algorithm for improving the ranking of search-query results, a single PageRank vector is computed, using the link structure of the Web, to capture the relative \"importance\" of Web pages, independent of any particular search query. To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic. By using these (precomputed) biased PageRank vectors to generate query-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector. For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords. For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>108719</td>\n",
       "      <td>2005.wwwconf_conference-2005si.147</td>\n",
       "      <td>9</td>\n",
       "      <td>15.303768</td>\n",
       "      <td>pagerank</td>\n",
       "      <td>BackRank: an alternative for PageRank?\\n\\n\\n ABSTRACTThis paper proposes to extend a previous work, The Effect of the Back Button in a Random Walk: Application for PageRank . We introduce an enhanced version of the PageRank algorithm using a realistic model for the Back button, thus improving the random surfer model. We show that in the special case where the history is bound to an unique page (you cannot use the Back button twice in a row), we can produce an algorithm that does not need much more resources than a standard PageRank. This algorithm, BackRank, can converge up to 30% faster than a standard PageRank and suppress most of the drawbacks induced by the existence of pages without links.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                                         docno  rank      score  \\\n",
       "0   1   80914            2008.sigirconf_conference-2008.179     0  16.740101   \n",
       "1   1  105982            2004.wwwconf_conference-2004at.161     1  15.963780   \n",
       "2   1  108644             2005.wwwconf_conference-2005si.72     2  15.854111   \n",
       "3   1   91900                   2002.ecir_conference-2002.5     3  15.834797   \n",
       "4   1  126869  2010.tois_journal-ir0anthology0volumeA28A3.1     4  15.834797   \n",
       "5   1   94262                  2008.cikm_conference-2008.30     5  15.811399   \n",
       "6   1  105935            2004.wwwconf_conference-2004at.114     6  15.660582   \n",
       "7   1  125218  2008.ipm_journal-ir0anthology0volumeA44A2.21     7  15.499952   \n",
       "8   1  108466               2002.wwwconf_conference-2002.50     8  15.480179   \n",
       "9   1  108719            2005.wwwconf_conference-2005si.147     9  15.303768   \n",
       "\n",
       "      query  \\\n",
       "0  pagerank   \n",
       "1  pagerank   \n",
       "2  pagerank   \n",
       "3  pagerank   \n",
       "4  pagerank   \n",
       "5  pagerank   \n",
       "6  pagerank   \n",
       "7  pagerank   \n",
       "8  pagerank   \n",
       "9  pagerank   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Local approximation of PageRank and reverse PageRank\\n\\n\\n ABSTRACTWe consider the problem of approximating the PageRank of a target node using only local information provided by a link server. We prove that local approximation of PageRank is feasible if and only if the graph has low in-degree and admits fast PageRank convergence. While natural graphs, such as the web graph, are abundant with high in-degree nodes, making local PageRank approximation too costly, we show that reverse natural graphs tend to have low indegree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally. Finally, we demonstrate the usefulness of Reverse PageRank in five different applications.  \n",
       "1                                                                                                                                                                                                                                                                                                Outlink estimation for pagerank computation under missing data\\n\\n\\n ABSTRACTThe enormity and rapid growth of the web-graph forces quantities such as its pagerank to be computed under missing information consisting of outlinks of pages that have not yet been crawled. This paper examines the role played by the size and distribution of this missing data in determining the accuracy of the computed pagerank, focusing on questions such as (i) the accuracy of pageranks under missing information, (ii) the size at which a crawl process may be aborted while still ensuring reasonable accuracy of pageranks, and (iii) algorithms to estimate pageranks under such missing information. The first couple of questions are addressed on the basis of certain simple bounds relating the expected distance between the true and computed pageranks and the size of the missing data. The third question is explored by devising algorithms to predict the pageranks when full information is not available. A key feature of the \"dangling link estimation\" and \"clustered link estimation\" algorithms proposed is that, they do not need to run the pagerank iteration afresh once the outlinks have been estimated.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    On the feasibility of low-rank approximation for personalized PageRank\\n\\n\\n ABSTRACTPersonalized PageRank expresses backlink-based page quality around user-selected pages in a similar way to PageRank over the entire Web. Algorithms for computing personalized PageRank on the fly are either limited to a restricted choice of page selection or believed to behave well only on sparser regions of the Web. In this paper we show the feasibility of computing personalized PageRank by a k < 1000 lowrank approximation of the PageRank transition matrix; by our algorithm we may compute an approximate personalized PageRank by multiplying an n × k, a k × n matrix and the n-dimensional personalization vector. Since low-rank approximations are accurate on dense regions, we hope that our technique will combine well with known algorithms.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  An Improved Computation of the PageRank Algorithm\\n\\n\\n Abstract. The Google search site (http://www.google.com) exploits the link structure of the Web to measure the relative importance of Web pages. The ranking method implemented in Google is called PageRank . The sum of all PageRank values should be one. However, we notice that the sum becomes less than one in some cases. We present an improved PageRank algorithm that computes the PageRank values of the Web pages correctly. Our algorithm works out well in any situations, and the sum of all PageRank values is always maintained to be one. We also present implementation issues of the improved algorithm. Experimental evaluation is carried out and the results are also discussed.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Arnoldi versus GMRES for computing pageRank: A theoretical contribution to google's pageRank problem\\n\\n\\n PageRank is one of the most important ranking techniques used in today's search engines. A recent very interesting research track focuses on exploiting efficient numerical methods to speed up the computation of PageRank, among which the Arnoldi-type algorithm and the GMRES algorithm are competitive candidates. In essence, the former deals with the PageRank problem from an eigenproblem, while the latter from a linear system, point of view. However, there is little known about the relations between the two approaches for PageRank. In this article, we focus on a theoretical and numerical comparison of the two approaches. Numerical experiments illustrate the effectiveness of our theoretical results.  \n",
       "5  Local approximation of pagerank and reverse pagerank\\n\\n\\n ABSTRACTWe consider the problem of approximating the PageRank of a target node using only local information provided by a link server. This problem was originally studied by Chen, Gan, and Suel (CIKM  2004), who presented an algorithm for tackling it. We prove that local approximation of PageRank, even to within modest approximation factors, is infeasible in the worst-case, as it requires probing the link server for Ω(n) nodes, where n is the size of the graph. The difficulty emanates from nodes of high in-degree and/or from slow convergence of the PageRank random walk.We show that when the graph has bounded in-degree and admits fast PageRank convergence, then local PageRank approximation can be done using a small number of queries. Unfortunately, natural graphs, such as the web graph, are abundant with high in-degree nodes, making this algorithm (or any other local approximation algorithm) too costly. On the other hand, reverse natural graphs tend to have low in-degree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally.We demonstrate that Reverse PageRank is useful for several applications, including computation of hub scores for web pages, finding influencers in social networks, obtaining good seeds for crawling, and measurement of semantic relatedness between concepts in a taxonomy.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                      Updating pagerank with iterative aggregation\\n\\n\\n ABSTRACTWe present an algorithm for updating the PageRank vector . Due to the scale of the web, Google only updates its famous PageRank vector on a monthly basis. However, the Web changes much more frequently. Drastically speeding the PageRank computation can lead to fresher, more accurate rankings of the webpages retrieved by search engines. It can also make the goal of real-time personalized rankings within reach. On two small subsets of the web, our algorithm updates PageRank using just 25% and 14%, respectively, of the time required by the original PageRank algorithm. Our algorithm uses iterative aggregation techniques  to focus on the slow-converging states of the Markov chain. The most exciting feature of this algorithm is that it can be joined with other PageRank acceleration methods, such as the dangling node lumpability algorithm [6], quadratic extrapolation , and adaptive PageRank , to realize even greater speedups (potentially a factor of 60 or more speedup when all algorithms are combined).  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Bringing PageRank to the citation analysis\\n\\n\\n AbstractThe paper attempts to provide an alternative method for measuring the importance of scientific papers based on the Google's PageRank. The method is a meaningful extension of the common integer counting of citations and is then experimented for bringing PageRank to the citation analysis in a large citation network. It offers a more integrated picture of the publications' influence in a specific field. We firstly calculate the PageRanks of scientific papers. The distributional characteristics and comparison with the traditionally used number of citations are then analyzed in detail. Furthermore, the PageRank is implemented in the evaluation of research influence for several countries in the field of Biochemistry and Molecular Biology during the time period of 2000-2005. Finally, some advantages of bringing PageRank to the citation analysis are concluded.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                  Topic-sensitive PageRank\\n\\n\\n ABSTRACTIn the original PageRank algorithm for improving the ranking of search-query results, a single PageRank vector is computed, using the link structure of the Web, to capture the relative \"importance\" of Web pages, independent of any particular search query. To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic. By using these (precomputed) biased PageRank vectors to generate query-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector. For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords. For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   BackRank: an alternative for PageRank?\\n\\n\\n ABSTRACTThis paper proposes to extend a previous work, The Effect of the Back Button in a Random Walk: Application for PageRank . We introduce an enhanced version of the PageRank algorithm using a realistic model for the Back button, thus improving the random surfer model. We show that in the special case where the history is bound to an unique page (you cannot use the Back button twice in a row), we can produce an algorithm that does not need much more resources than a standard PageRank. This algorithm, BackRank, can converge up to 30% faster than a standard PageRank and suppress most of the drawbacks induced by the existence of pages without links.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search('pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmisinformation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/transformer.py:169\u001b[0m, in \u001b[0;36mTransformer.search\u001b[0;34m(self, query, qid, sort)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m    168\u001b[0m queryDf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([[qid, query]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 169\u001b[0m rtr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryDf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    171\u001b[0m     rtr \u001b[38;5;241m=\u001b[39m rtr\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/apply_base.py:254\u001b[0m, in \u001b[0;36mApplyGenericTransformer.transform\u001b[0;34m(self, inputRes)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputRes):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# no batching\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputRes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# batching\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/text.py:122\u001b[0m, in \u001b[0;36m_add_text_irds_docstore.<locals>.add_text_function_docids\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m    120\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m {f: [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(docids) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m metadata}\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docstore\u001b[38;5;241m.\u001b[39mget_many_iter(docids):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m didx \u001b[38;5;129;01min\u001b[39;00m did2idxs[\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_id\u001b[49m]:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f, fidx \u001b[38;5;129;01min\u001b[39;00m field_idx:\n\u001b[1;32m    124\u001b[0m             new_columns[f][didx] \u001b[38;5;241m=\u001b[39m doc[fidx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'doc_id'"
     ]
    }
   ],
   "source": [
    "bm25.search('misinformation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
