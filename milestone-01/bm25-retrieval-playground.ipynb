{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, ir_datasets\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/anthology-20240408-training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/2024-04-08-15-52-07.zip\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 19.5M/19.5M [00:00<00:00, 29.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-lab-sose-2024/anthology-20240408-training/tira-ir-starter\n"
     ]
    }
   ],
   "source": [
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/anthology-20240408-inputs.zip?download=1\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 37.9M/37.9M [00:01<00:00, 30.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_datasets/ir-lab-sose-2024/anthology-20240408-training/\n"
     ]
    }
   ],
   "source": [
    "# Declarative pipeline:\n",
    "# Step 1: retrieve the top 10 results with BM25\n",
    "# Step 2: Add the document text\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\") %10 >> pt.text.get_text(pt_dataset, \"text\")\n",
    "\n",
    "# do not truncate text in the dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>109983</td>\n",
       "      <td>2015.trec_conference-2015.22</td>\n",
       "      <td>0</td>\n",
       "      <td>19.503892</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>EMSE at TREC 2015 Clinical Decision Support Track\\n\\n\\n This paper describes the participation of the EMSE team at the clinical decision support track of TREC 2015 (Task A). Our team submitted three automatic runs based only on the summary field. The baseline run uses the summary field as a query and the query likelihood retrieval model to match articles. Other runs explore different approaches to expand the summary field: RM3, LSI with pseudo relevant documents, semantic ressources of UMLS, and a hybrid approach called SMERA that combines LSI and UMLS based approaches. Only three of our runs were considered for the 2015 campaign: RM3, LSI and SMERA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28757</td>\n",
       "      <td>2021.nlp4prog-1.10</td>\n",
       "      <td>1</td>\n",
       "      <td>18.665524</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>Bag-of-Words Baselines for Semantic Code Search\\n\\n\\n The task of semantic code search is to retrieve code snippets from a source code corpus based on an information need expressed in natural language. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that \"traditional\" bag-of-words IR methods are poorly suited for semantic code search: our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely BM25 and RM3, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness. Code for reproducing our experiments is available at https: //github.com/crystina-z/CodeSearch Net-baseline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>94720</td>\n",
       "      <td>2009.cikm_conference-2009.247</td>\n",
       "      <td>2</td>\n",
       "      <td>15.882512</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>A machine learning approach for improved BM25 retrieval\\n\\n\\n ABSTRACTDespite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations. We determine the effectiveness of BM25 on various document fields. We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes. We also find query click information to be the single most important field for retrieval. In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25. Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures. We demonstrate the advantages of our model on a very large real-world Web data collection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87286</td>\n",
       "      <td>2019.fire_conference-2019w.7</td>\n",
       "      <td>3</td>\n",
       "      <td>15.231518</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>FIRE2019@AILA: Legal Information Retrieval Using Improved BM25\\n\\n\\n This paper details the approaches of implementing the tasks of identifying relevant precedents and identifying relevant statues in the evaluation of Artificial Intelligence for Legal Assistance proposed by Forum of Information Retrieval Evaluation in 2019(AILA@Fire2019). We formalize the two tasks as the issue of information retrieval, and present the improved BM25 models to retrieve the prior cases and identify the relevant statues. For the task of identifying relevant precedents, the proposed improved BM25 model integrates the relevance scores of the original current case and the filtered current case. For the task of identifying relevant statues, the proposed improved BM25 models exploit the search results as the reference documents of the current case and integrate the ranking information of search results into the BM25 model. Comparisons to the other submissions for the same tasks, our improved BM25 model achieves the top performers for the task of identifying relevant precedents on all evaluation measures. For the task of identifying relevant statues, the improved BM25 model wins the second place on 1/rank of first relevant document and the third place on BPREF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>79623</td>\n",
       "      <td>2011.sigirconf_conference-2011.121</td>\n",
       "      <td>4</td>\n",
       "      <td>14.919702</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>When documents are very long, BM25 fails!\\n\\n\\n ABSTRACTWe reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which \"shifts\" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]: Retrieval models\\nGeneral TermsAlgorithms Keywords BM25, BM25L, term frequency, very long documents\\nMOTIVATIONThe Okapi BM25 retrieval function  has been the state-of-the-art for nearly two decades. BM25 scores a document D with respect to query Q as follows:where c(q, Q) is the count of q in Q, N is the total number of documents, df (q) is the document frequency of q, and k3 is a parameter. Following [1], we use a modified IDF formula in BM25 to avoid its problem of possibly negative IDF values. A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula:where |D| represents document length, avdl stands for average document length, c(q, D) is the raw TF of q in D, and b and k1 are two parameters. c (q, D) is the normalized TF by document length using pivoted length normalization .Copyright is held by the author/owner(s). SIGIR'11, July 24-28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07. \\nBOOSTING VERY LONG DOCUMENTSIn order to avoid overly-penalizing very long documents, we need to add a constraint in TF normalization to make sure that the \"score gap\" of f One heuristic way to achieve this goal is to define f (q, D) as follows:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>91659</td>\n",
       "      <td>2020.ecir_conference-20202.4</td>\n",
       "      <td>5</td>\n",
       "      <td>14.570957</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants\\n\\n\\n When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.'s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \"matter\"? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene's often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>78722</td>\n",
       "      <td>2015.iir_workshop-2015.14</td>\n",
       "      <td>6</td>\n",
       "      <td>14.455476</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>Geometric Perspectives of the BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>78816</td>\n",
       "      <td>2013.iir_workshop-2013.1</td>\n",
       "      <td>7</td>\n",
       "      <td>14.455476</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>Are There New BM25 Expectations?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>81827</td>\n",
       "      <td>2006.sigirconf_conference-2006.90</td>\n",
       "      <td>8</td>\n",
       "      <td>14.439580</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>Term proximity scoring for ad-hoc retrieval on very large text collections\\n\\n\\n ABSTRACTWe propose an integration of term proximity scoring into Okapi BM25. The relative retrieval effectiveness of our retrieval method, compared to pure BM25, varies from collection to collection. We present an experimental evaluation of our method and show that the gains achieved over BM25 grow as the size of the underlying text collection increases. We also show that for stemmed queries the impact of term proximity scoring is larger than for unstemmed queries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>110064</td>\n",
       "      <td>2012.trec_conference-2012.15</td>\n",
       "      <td>9</td>\n",
       "      <td>14.346375</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>DCU@TRECMed 2012: Using adhoc Baselines for Domain-Specific Retrieval\\n\\n\\n This paper describes the first participation of DCU in the TREC Medical Records Track (TRECMed) 2012. We performed initial experiments on the the 2011 TRECMed data based on the BM25 retrieval model. Surprisingly, we found that the standard BM25 model with default parameters performs comparable to the best automatic runs submitted to TRECMed 2011 and our experiments would have ranked among the top four out of 29 participating groups. We expected that some form of domain adaptation would increase performance. However, results on the 2011 data proved otherwise: query expansion decreased performance, and filtering and reranking by term proximity also decreased performance slightly. We submitted four runs based on the BM25 retrieval model to TRECMed 2012 using standard BM25, standard query expansion, result filtering, and concept-based query expansion. Official results for 2012 confirm that domain-specific knowledge, as applied by us, does not increase performance compared to the BM25 baseline.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score     query  \\\n",
       "0   1  109983        2015.trec_conference-2015.22     0  19.503892  bm25 rm3   \n",
       "1   1   28757                  2021.nlp4prog-1.10     1  18.665524  bm25 rm3   \n",
       "2   1   94720       2009.cikm_conference-2009.247     2  15.882512  bm25 rm3   \n",
       "3   1   87286        2019.fire_conference-2019w.7     3  15.231518  bm25 rm3   \n",
       "4   1   79623  2011.sigirconf_conference-2011.121     4  14.919702  bm25 rm3   \n",
       "5   1   91659        2020.ecir_conference-20202.4     5  14.570957  bm25 rm3   \n",
       "6   1   78722           2015.iir_workshop-2015.14     6  14.455476  bm25 rm3   \n",
       "7   1   78816            2013.iir_workshop-2013.1     7  14.455476  bm25 rm3   \n",
       "8   1   81827   2006.sigirconf_conference-2006.90     8  14.439580  bm25 rm3   \n",
       "9   1  110064        2012.trec_conference-2012.15     9  14.346375  bm25 rm3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              EMSE at TREC 2015 Clinical Decision Support Track\\n\\n\\n This paper describes the participation of the EMSE team at the clinical decision support track of TREC 2015 (Task A). Our team submitted three automatic runs based only on the summary field. The baseline run uses the summary field as a query and the query likelihood retrieval model to match articles. Other runs explore different approaches to expand the summary field: RM3, LSI with pseudo relevant documents, semantic ressources of UMLS, and a hybrid approach called SMERA that combines LSI and UMLS based approaches. Only three of our runs were considered for the 2015 campaign: RM3, LSI and SMERA.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Bag-of-Words Baselines for Semantic Code Search\\n\\n\\n The task of semantic code search is to retrieve code snippets from a source code corpus based on an information need expressed in natural language. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that \"traditional\" bag-of-words IR methods are poorly suited for semantic code search: our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely BM25 and RM3, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness. Code for reproducing our experiments is available at https: //github.com/crystina-z/CodeSearch Net-baseline.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                A machine learning approach for improved BM25 retrieval\\n\\n\\n ABSTRACTDespite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations. We determine the effectiveness of BM25 on various document fields. We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes. We also find query click information to be the single most important field for retrieval. In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25. Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures. We demonstrate the advantages of our model on a very large real-world Web data collection.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         FIRE2019@AILA: Legal Information Retrieval Using Improved BM25\\n\\n\\n This paper details the approaches of implementing the tasks of identifying relevant precedents and identifying relevant statues in the evaluation of Artificial Intelligence for Legal Assistance proposed by Forum of Information Retrieval Evaluation in 2019(AILA@Fire2019). We formalize the two tasks as the issue of information retrieval, and present the improved BM25 models to retrieve the prior cases and identify the relevant statues. For the task of identifying relevant precedents, the proposed improved BM25 model integrates the relevance scores of the original current case and the filtered current case. For the task of identifying relevant statues, the proposed improved BM25 models exploit the search results as the reference documents of the current case and integrate the ranking information of search results into the BM25 model. Comparisons to the other submissions for the same tasks, our improved BM25 model achieves the top performers for the task of identifying relevant precedents on all evaluation measures. For the task of identifying relevant statues, the improved BM25 model wins the second place on 1/rank of first relevant document and the third place on BPREF.  \n",
       "4  When documents are very long, BM25 fails!\\n\\n\\n ABSTRACTWe reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which \"shifts\" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]: Retrieval models\\nGeneral TermsAlgorithms Keywords BM25, BM25L, term frequency, very long documents\\nMOTIVATIONThe Okapi BM25 retrieval function  has been the state-of-the-art for nearly two decades. BM25 scores a document D with respect to query Q as follows:where c(q, Q) is the count of q in Q, N is the total number of documents, df (q) is the document frequency of q, and k3 is a parameter. Following [1], we use a modified IDF formula in BM25 to avoid its problem of possibly negative IDF values. A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula:where |D| represents document length, avdl stands for average document length, c(q, D) is the raw TF of q in D, and b and k1 are two parameters. c (q, D) is the normalized TF by document length using pivoted length normalization .Copyright is held by the author/owner(s). SIGIR'11, July 24-28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07. \\nBOOSTING VERY LONG DOCUMENTSIn order to avoid overly-penalizing very long documents, we need to add a constraint in TF normalization to make sure that the \"score gap\" of f One heuristic way to achieve this goal is to define f (q, D) as follows:  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants\\n\\n\\n When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.'s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \"matter\"? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene's often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Geometric Perspectives of the BM25  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Are There New BM25 Expectations?  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Term proximity scoring for ad-hoc retrieval on very large text collections\\n\\n\\n ABSTRACTWe propose an integration of term proximity scoring into Okapi BM25. The relative retrieval effectiveness of our retrieval method, compared to pure BM25, varies from collection to collection. We present an experimental evaluation of our method and show that the gains achieved over BM25 grow as the size of the underlying text collection increases. We also show that for stemmed queries the impact of term proximity scoring is larger than for unstemmed queries.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        DCU@TRECMed 2012: Using adhoc Baselines for Domain-Specific Retrieval\\n\\n\\n This paper describes the first participation of DCU in the TREC Medical Records Track (TRECMed) 2012. We performed initial experiments on the the 2011 TRECMed data based on the BM25 retrieval model. Surprisingly, we found that the standard BM25 model with default parameters performs comparable to the best automatic runs submitted to TRECMed 2011 and our experiments would have ranked among the top four out of 29 participating groups. We expected that some form of domain adaptation would increase performance. However, results on the 2011 data proved otherwise: query expansion decreased performance, and filtering and reranking by term proximity also decreased performance slightly. We submitted four runs based on the BM25 retrieval model to TRECMed 2012 using standard BM25, standard query expansion, result filtering, and concept-based query expansion. Official results for 2012 confirm that domain-specific knowledge, as applied by us, does not increase performance compared to the BM25 baseline.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search('bm25 rm3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>108359</td>\n",
       "      <td>2001.wwwconf_conference-2001p.43</td>\n",
       "      <td>0</td>\n",
       "      <td>26.825083</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Improving Web Site's Accessibility\\n\\n\\n We consider the problem of improving the performance of web access by proposing a reconstruction of the internal link structure of a web site in order to match the quality of the pages (measured in terms of their link importance in the web space) with the popularity of the pages (measured in terms of their importance recognized by web users). We provide a set of simple algorithms in order to increase the access rate of popular pages by using local reorganization of the web site's pages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>105767</td>\n",
       "      <td>2004.wwwconf_conference-2004.21</td>\n",
       "      <td>1</td>\n",
       "      <td>24.812344</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Learning block importance models for web pages\\n\\n\\n ABSTRACTPrevious work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>101428</td>\n",
       "      <td>2003.spire_conference-2003.4</td>\n",
       "      <td>2</td>\n",
       "      <td>24.786592</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Link Information as a Similarity Measure in Web Classification\\n\\n\\n Abstract. The objective of this paper is to study how the link structure of the Web can be used to derive a similarity measure between documents. We evaluate five different measures and determine how accurate they are in predicting the subject of Web pages. Experiments with a Web directory indicate that the use of links from external pages greatly increases the quality of the results. Gains as high as 45.9 points in F1 were obtained, when compared to a text-based classifier. Among the similarity measures tested in this work, co-citation presented the best performance in determining if two Web pages are related. This work provides an important insight on how similarity measures can be derived from links and applied to Web IR problems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>92065</td>\n",
       "      <td>2005.ecir_conference-2005.35</td>\n",
       "      <td>3</td>\n",
       "      <td>24.769014</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Factors Affecting Web Page Similarity\\n\\n\\n Abstract. Tools that allow effective information organisation, access and navigation are becoming increasingly important on the Web. Similarity between web pages is a concept that is central to such tools. In this paper, we examine the effect that content and layout-related aspects of web pages have on web page similarity. We consider the textual content contained within common HTML tags, the structural layout of pages, and the query terms contained within pages. Our study shows that combinations of factors can yield more promising results than individual factors, and that different aspects of web pages affect similarities between pages in a different manner. We found a number of factors that, when taken into account, can result in effective measures of similarity between web pages. Query information in particular, proved to be important for the effective organisation of web pages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>107044</td>\n",
       "      <td>2006.wwwconf_conference-2006.25</td>\n",
       "      <td>4</td>\n",
       "      <td>24.121647</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>What's really new on the web?: identifying new pages from a series of unstable web snapshots\\n\\n\\n ABSTRACTIdentifying and tracking new information on the Web is important in sociology, marketing, and survey research, since new trends might be apparent in the new information. Such changes can be observed by crawling the Web periodically. In practice, however, it is impossible to crawl the entire expanding Web repeatedly. This means that the novelty of a page remains unknown, even if that page did not exist in previous snapshots. In this paper, we propose a novelty measure for estimating the certainty that a newly crawled page appeared between the previous and current crawls. Using this novelty measure, new pages can be extracted from a series of unstable snapshots for further analysis and mining to identify new trends on the Web. We evaluated the precision, recall, and miss rate of the novelty measure using our Japanese web archive, and applied it to a Web archive search engine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82293</td>\n",
       "      <td>2010.sigirconf_conference-2010.181</td>\n",
       "      <td>5</td>\n",
       "      <td>23.639222</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Capturing page freshness for web search\\n\\n\\n ABSTRACTFreshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives-the page itself and its in-linked pages-and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a realworld archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>96057</td>\n",
       "      <td>2007.cikm_conference-2007.130</td>\n",
       "      <td>6</td>\n",
       "      <td>23.426529</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Link analysis using time series of web graphs\\n\\n\\n ABSTRACTLink analysis is a key technology in contemporary web search engines. Most of the previous work on link analysis only used information from one snapshot of web graph. Since commercial search engines crawl the Web periodically, they will naturally obtain time series data of web graphs. The historical information contained in the series of web graphs can be used to improve the performance of link analysis. In this paper, we argue that page importance should be a dynamic quantity, and propose defining page importance as a function of both PageRank of the current web graph and accumulated historical page importance from previous web graphs. Specifically, a novel algorithm named TemporalRank is designed to compute the proposed page importance. We try to use a kinetic model to interpret this page importance and show that it can be regarded as the solution to an ordinary differential equation. Experiments on link analysis using web graph data in five snapshots show that the proposed algorithm can outperform PageRank in many measures, and can effectively filter out newly appeared link spam websites.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>108679</td>\n",
       "      <td>2005.wwwconf_conference-2005si.107</td>\n",
       "      <td>7</td>\n",
       "      <td>23.372102</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>A study on combination of block importance and relevance to estimate page relevance\\n\\n\\n ABSTRACTSome work showed that segmenting web pages into \"semantic independent\" blocks could help to improve the whole page retrieval. One key and unexplored issue is how to combine the block importance and relevance to a given query. In this poster, we first propose an automatic way to measure block importance to improve retrieval. After that, user information need is also concerned to refine block importance for different users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>92452</td>\n",
       "      <td>2011.cikm_conference-2011.12</td>\n",
       "      <td>8</td>\n",
       "      <td>23.337013</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>User browsing behavior-driven web crawling\\n\\n\\n ABSTRACTTo optimize the performance of web crawlers, various measures of page importance have been studied to select and order URLs in crawling. Most sophisticated measures (e.g. breadth-first and PageRank ) are based on link structure. In this paper, we treat the problem from another perspective and propose to directly measure page importance through mining user interest and behaviors from web browse logs. Unlike most existing approaches which work on single URL, in this paper, both the log mining and the crawl ordering are performed at the granularity of URL pattern. The proposed URL pattern-based crawl orderings are capable to properly predict the importance of newly created (unseen) URLs. Promising experimental results proved the feasibility of our approach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>19409</td>\n",
       "      <td>W18-3721</td>\n",
       "      <td>9</td>\n",
       "      <td>23.050681</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>Measuring Beginner Friendliness of {J}apanese Web Pages explaining Academic Concepts by Integrating Neural Image Feature and Text Features\\n\\n\\n Search engine is an important tool of modern academic study, but the results are lack of measurement of beginner friendliness. In order to improve the efficiency of using search engine for academic study, it is necessary to invent a technique of measuring the beginner friendliness of a Web page explaining academic concepts and to build an automatic measurement system. This paper studies how to integrate heterogeneous features such as a neural image feature generated from the image of the Web page by a variant of CNN (convolutional neural network) as well as text features extracted from the body text of the HTML file of the Web page. Integration is performed through the framework of the SVM classifier learning. Evaluation results show that heterogeneous features perform better than each individual type of features.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1  108359    2001.wwwconf_conference-2001p.43     0  26.825083   \n",
       "1   1  105767     2004.wwwconf_conference-2004.21     1  24.812344   \n",
       "2   1  101428        2003.spire_conference-2003.4     2  24.786592   \n",
       "3   1   92065        2005.ecir_conference-2005.35     3  24.769014   \n",
       "4   1  107044     2006.wwwconf_conference-2006.25     4  24.121647   \n",
       "5   1   82293  2010.sigirconf_conference-2010.181     5  23.639222   \n",
       "6   1   96057       2007.cikm_conference-2007.130     6  23.426529   \n",
       "7   1  108679  2005.wwwconf_conference-2005si.107     7  23.372102   \n",
       "8   1   92452        2011.cikm_conference-2011.12     8  23.337013   \n",
       "9   1   19409                            W18-3721     9  23.050681   \n",
       "\n",
       "                             query  \\\n",
       "0  measure importance of web pages   \n",
       "1  measure importance of web pages   \n",
       "2  measure importance of web pages   \n",
       "3  measure importance of web pages   \n",
       "4  measure importance of web pages   \n",
       "5  measure importance of web pages   \n",
       "6  measure importance of web pages   \n",
       "7  measure importance of web pages   \n",
       "8  measure importance of web pages   \n",
       "9  measure importance of web pages   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Improving Web Site's Accessibility\\n\\n\\n We consider the problem of improving the performance of web access by proposing a reconstruction of the internal link structure of a web site in order to match the quality of the pages (measured in terms of their link importance in the web space) with the popularity of the pages (measured in terms of their importance recognized by web users). We provide a set of simple algorithms in order to increase the access rate of popular pages by using local reorganization of the web site's pages.  \n",
       "1  Learning block importance models for web pages\\n\\n\\n ABSTRACTPrevious work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Link Information as a Similarity Measure in Web Classification\\n\\n\\n Abstract. The objective of this paper is to study how the link structure of the Web can be used to derive a similarity measure between documents. We evaluate five different measures and determine how accurate they are in predicting the subject of Web pages. Experiments with a Web directory indicate that the use of links from external pages greatly increases the quality of the results. Gains as high as 45.9 points in F1 were obtained, when compared to a text-based classifier. Among the similarity measures tested in this work, co-citation presented the best performance in determining if two Web pages are related. This work provides an important insight on how similarity measures can be derived from links and applied to Web IR problems.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                     Factors Affecting Web Page Similarity\\n\\n\\n Abstract. Tools that allow effective information organisation, access and navigation are becoming increasingly important on the Web. Similarity between web pages is a concept that is central to such tools. In this paper, we examine the effect that content and layout-related aspects of web pages have on web page similarity. We consider the textual content contained within common HTML tags, the structural layout of pages, and the query terms contained within pages. Our study shows that combinations of factors can yield more promising results than individual factors, and that different aspects of web pages affect similarities between pages in a different manner. We found a number of factors that, when taken into account, can result in effective measures of similarity between web pages. Query information in particular, proved to be important for the effective organisation of web pages.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                              What's really new on the web?: identifying new pages from a series of unstable web snapshots\\n\\n\\n ABSTRACTIdentifying and tracking new information on the Web is important in sociology, marketing, and survey research, since new trends might be apparent in the new information. Such changes can be observed by crawling the Web periodically. In practice, however, it is impossible to crawl the entire expanding Web repeatedly. This means that the novelty of a page remains unknown, even if that page did not exist in previous snapshots. In this paper, we propose a novelty measure for estimating the certainty that a newly crawled page appeared between the previous and current crawls. Using this novelty measure, new pages can be extracted from a series of unstable snapshots for further analysis and mining to identify new trends on the Web. We evaluated the precision, recall, and miss rate of the novelty measure using our Japanese web archive, and applied it to a Web archive search engine.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                        Capturing page freshness for web search\\n\\n\\n ABSTRACTFreshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives-the page itself and its in-linked pages-and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a realworld archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.  \n",
       "6                                                                                                                                                                                               Link analysis using time series of web graphs\\n\\n\\n ABSTRACTLink analysis is a key technology in contemporary web search engines. Most of the previous work on link analysis only used information from one snapshot of web graph. Since commercial search engines crawl the Web periodically, they will naturally obtain time series data of web graphs. The historical information contained in the series of web graphs can be used to improve the performance of link analysis. In this paper, we argue that page importance should be a dynamic quantity, and propose defining page importance as a function of both PageRank of the current web graph and accumulated historical page importance from previous web graphs. Specifically, a novel algorithm named TemporalRank is designed to compute the proposed page importance. We try to use a kinetic model to interpret this page importance and show that it can be regarded as the solution to an ordinary differential equation. Experiments on link analysis using web graph data in five snapshots show that the proposed algorithm can outperform PageRank in many measures, and can effectively filter out newly appeared link spam websites.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A study on combination of block importance and relevance to estimate page relevance\\n\\n\\n ABSTRACTSome work showed that segmenting web pages into \"semantic independent\" blocks could help to improve the whole page retrieval. One key and unexplored issue is how to combine the block importance and relevance to a given query. In this poster, we first propose an automatic way to measure block importance to improve retrieval. After that, user information need is also concerned to refine block importance for different users.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          User browsing behavior-driven web crawling\\n\\n\\n ABSTRACTTo optimize the performance of web crawlers, various measures of page importance have been studied to select and order URLs in crawling. Most sophisticated measures (e.g. breadth-first and PageRank ) are based on link structure. In this paper, we treat the problem from another perspective and propose to directly measure page importance through mining user interest and behaviors from web browse logs. Unlike most existing approaches which work on single URL, in this paper, both the log mining and the crawl ordering are performed at the granularity of URL pattern. The proposed URL pattern-based crawl orderings are capable to properly predict the importance of newly created (unseen) URLs. Promising experimental results proved the feasibility of our approach.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                     Measuring Beginner Friendliness of {J}apanese Web Pages explaining Academic Concepts by Integrating Neural Image Feature and Text Features\\n\\n\\n Search engine is an important tool of modern academic study, but the results are lack of measurement of beginner friendliness. In order to improve the efficiency of using search engine for academic study, it is necessary to invent a technique of measuring the beginner friendliness of a Web page explaining academic concepts and to build an automatic measurement system. This paper studies how to integrate heterogeneous features such as a neural image feature generated from the image of the Web page by a variant of CNN (convolutional neural network) as well as text features extracted from the body text of the HTML file of the Web page. Integration is performed through the framework of the SVM classifier learning. Evaluation results show that heterogeneous features perform better than each individual type of features.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search('measure importance of web pages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
