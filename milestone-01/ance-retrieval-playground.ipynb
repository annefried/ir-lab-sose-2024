{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, ir_datasets\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/anthology-20240408-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/2024-04-08-18-00-08.zip\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 344M/344M [00:11<00:00, 32.1MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Extract...\n",
      "Extraction finished:  /root/.tira/extracted_runs/ir-lab-sose-2024/anthology-20240408-training/ows\n",
      "Download from the Incubator: https://files.webis.de/data-in-production/data-research/tira-zenodo-dump-preparation/ir-lab-sose2024/Passage_ANCE_FirstP_Checkpoint.zip\n",
      "\tThis is only used for last spot checks before archival to Zenodo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 1.19G/1.19G [00:40<00:00, 31.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finished. Persist...\n",
      "Download finished:  /root/.tira/raw_resources/Passage_ANCE_FirstP_Checkpoint.zip\n",
      "Extracting checkpoint /root/.tira/raw_resources/Passage_ANCE_FirstP_Checkpoint.zip\n",
      "Loading checkpoint /tmp/tmpbtnh8q82/Passage ANCE(FirstP) Checkpoint\n",
      "Using mean: False\n",
      "Loading shard metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading shards: 100%|██████████| 1/1 [00:00<00:00,  2.81shard/s]\n"
     ]
    }
   ],
   "source": [
    "# Declarative pipeline:\n",
    "# Step 1: retrieve the top 10 results with ANCE\n",
    "# Step 2: Add the document text\n",
    "pt_ance = tira.pt_ance.ance_retrieval(pt_dataset)\n",
    "\n",
    "ance = pt_ance %10 >> pt.text.get_text(pt_dataset, \"text\")\n",
    "\n",
    "# do not truncate text in the dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 1 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n",
      "***** faiss search for 1 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.68shard/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>79623</td>\n",
       "      <td>2011.sigirconf_conference-2011.121</td>\n",
       "      <td>709.886841</td>\n",
       "      <td>0</td>\n",
       "      <td>When documents are very long, BM25 fails!\\n\\n\\n ABSTRACTWe reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which \"shifts\" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]: Retrieval models\\nGeneral TermsAlgorithms Keywords BM25, BM25L, term frequency, very long documents\\nMOTIVATIONThe Okapi BM25 retrieval function  has been the state-of-the-art for nearly two decades. BM25 scores a document D with respect to query Q as follows:where c(q, Q) is the count of q in Q, N is the total number of documents, df (q) is the document frequency of q, and k3 is a parameter. Following [1], we use a modified IDF formula in BM25 to avoid its problem of possibly negative IDF values. A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula:where |D| represents document length, avdl stands for average document length, c(q, D) is the raw TF of q in D, and b and k1 are two parameters. c (q, D) is the normalized TF by document length using pivoted length normalization .Copyright is held by the author/owner(s). SIGIR'11, July 24-28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07. \\nBOOSTING VERY LONG DOCUMENTSIn order to avoid overly-penalizing very long documents, we need to add a constraint in TF normalization to make sure that the \"score gap\" of f One heuristic way to achieve this goal is to define f (q, D) as follows:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>94817</td>\n",
       "      <td>2004.cikm_conference-2004.6</td>\n",
       "      <td>708.151855</td>\n",
       "      <td>1</td>\n",
       "      <td>Simple BM25 extension to multiple weighted fields\\n\\n\\n ABSTRACTThis paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the nonlinear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>110064</td>\n",
       "      <td>2012.trec_conference-2012.15</td>\n",
       "      <td>708.033813</td>\n",
       "      <td>2</td>\n",
       "      <td>DCU@TRECMed 2012: Using adhoc Baselines for Domain-Specific Retrieval\\n\\n\\n This paper describes the first participation of DCU in the TREC Medical Records Track (TRECMed) 2012. We performed initial experiments on the the 2011 TRECMed data based on the BM25 retrieval model. Surprisingly, we found that the standard BM25 model with default parameters performs comparable to the best automatic runs submitted to TRECMed 2011 and our experiments would have ranked among the top four out of 29 participating groups. We expected that some form of domain adaptation would increase performance. However, results on the 2011 data proved otherwise: query expansion decreased performance, and filtering and reranking by term proximity also decreased performance slightly. We submitted four runs based on the BM25 retrieval model to TRECMed 2012 using standard BM25, standard query expansion, result filtering, and concept-based query expansion. Official results for 2012 confirm that domain-specific knowledge, as applied by us, does not increase performance compared to the BM25 baseline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>101702</td>\n",
       "      <td>2018.ictir_conference-2018.36</td>\n",
       "      <td>708.010620</td>\n",
       "      <td>3</td>\n",
       "      <td>StatBM25: An Aggregative and Statistical Approach for Document Ranking\\n\\n\\n ABSTRACTIn Information Retrieval and Web Search, BM25 is one of the most influential probabilistic retrieval formulas for document weighting and ranking. BM25 involves three parameters k 1 , k 3 and b, which provide scalar approximation and scaling of important document features such as term frequency, document frequency, and document length. We investigate in this paper aggregative and statistical document features for document ranking. Shortly speaking, a statistically adjusted BM25 is used to score in an aggregative way on virtual documents, which are generated by randomly combining documents from the original collection. The problem size, in the number of virtual documents to be ranked, is an expansion to the problem size of the original problem. As a result, ranking is actually realized through performing statistical sampling. Rejection Sampling, a simple Monte Carlo sampling method is used at present. This new framework is called StatBM25, in emphasizing first the fact that the original problem domain space is K-expanded (a concept to be further explained in the paper); Further, statistical sampling is employed in the model. Empirical studies are performed on several standard test collections, where StatBM25 demonstrates convincingly high degree of both uniqueness and effectiveness compared to BM25. This means, in our belief, that StatBM25 as a statistically smoothed and normalized variant to BM25, might eventually lead to discoveries of useful new statistic measures for document ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>102200</td>\n",
       "      <td>2012.wwwconf_conference-2012c.74</td>\n",
       "      <td>707.888000</td>\n",
       "      <td>4</td>\n",
       "      <td>H2RDF: adaptive query processing on RDF data in the cloud\\n\\n\\n ABSTRACTIn this work we present H2RDF , a fully distributed RDF store that combines the MapReduce processing framework with a NoSQL distributed data store. Our system features two unique characteristics that enable efficient processing of both simple and multi-join SPARQL queries on virtually unlimited number of triples: Join algorithms that execute joins according to query selectivity to reduce processing; and adaptive choice among centralized and distributed (MapReduce-based) join execution for fast query responses. Our system efficiently answers both simple joins and complex multivariate queries and easily scales to 3 billion triples using a small cluster of 9 worker nodes. H2RDF outperforms state-of-the-art distributed solutions in multi-join and nonselective queries while achieving comparable performance to centralized solutions in selective queries. In this demonstration we showcase the system's functionality through an interactive GUI. Users will be able to execute predefined or custom-made SPARQL queries on datasets of different sizes, using different join algorithms. Moreover, they can repeat all queries utilizing a different number of cluster resources. Using real-time cluster monitoring and detailed statistics, participants will be able to understand the advantages of different execution schemes versus the input data as well as the scalability properties of H2RDF over both the data size and the available worker resources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>126534</td>\n",
       "      <td>2017.tois_journal-ir0anthology0volumeA36A2.11</td>\n",
       "      <td>707.834106</td>\n",
       "      <td>5</td>\n",
       "      <td>Local Representative-Based Matrix Factorization for Cold-Start Recommendation\\n\\n\\n Cold-start recommendation is one of the most challenging problems in recommender systems. An important approach to cold-start recommendation is to conduct an interview for new users, called the interview-based approach. Among the interview-based methods, Representative-Based Matrix Factorization (RBMF)  provides an effective solution with appealing merits: it represents users over selected representative items, which makes the recommendations highly intuitive and interpretable. However, RBMF only utilizes a global set of representative items to model all users. Such a representation is somehow too strict and may not be flexible enough to capture varying users' interests. To address this problem, we propose a novel interview-based model to dynamically create meaningful user groups using decision trees and then select local representative items for different groups. A two-round interview is performed for a new user. In the first round, l 1 global questions are issued for group division, while in the second round, l 2 local-group-specific questions are given to derive local representation. We collect the feedback on the (l 1 + l 2 ) items to learn the user representations. By putting these steps together, we develop a joint optimization model, named local representative-based matrix factorization, for new user recommendations. Extensive experiments on three public datasets have demonstrated the effectiveness of the proposed model compared with several competitive baselines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>53598</td>\n",
       "      <td>P13-2109</td>\n",
       "      <td>707.796265</td>\n",
       "      <td>6</td>\n",
       "      <td>Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers\\n\\n\\n We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD 3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>94788</td>\n",
       "      <td>2009.cikm_conference-2009.315</td>\n",
       "      <td>707.705261</td>\n",
       "      <td>7</td>\n",
       "      <td>HDDBrs middleware for implementing highly available distributed databases\\n\\n\\n ABSTRACTOur demo presents HDDB RS , a middle tier offering to clients a highly available distributed database interface using Reed Solomon codes to compute parity data. Parity data is stored in dedicated parity DB backends, is synchronously updated and allows recovering from multiple DB backend unavailability. HDDB RS middle tier is implemented in JAVA using standard technology, and is designed to be interoperable with any database engine that provides a JDBC driver and implements X/open XA protocol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>41184</td>\n",
       "      <td>W19-4331</td>\n",
       "      <td>707.695557</td>\n",
       "      <td>8</td>\n",
       "      <td>Modality-based Factorization for Multimodal Fusion\\n\\n\\n We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M + 1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>bm25 rm3</td>\n",
       "      <td>110712</td>\n",
       "      <td>2006.trec_conference-2006.51</td>\n",
       "      <td>707.596924</td>\n",
       "      <td>9</td>\n",
       "      <td>MG4J at TREC 2006\\n\\n\\n MG4J participated in the ad hoc task of the Terabyte Track (find all the relevant documents with high precision from 25.2 million pages from the .gov domain) at TREC 2006. It was the second time the MG4J group participated to TREC. For this year, we integrated standard techniques (such as stemming and BM25 scoring) into MG4J, and submitted also automatic runs based on trivial query expansion techniques.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid     query   docid                                          docno  \\\n",
       "0   1  bm25 rm3   79623             2011.sigirconf_conference-2011.121   \n",
       "1   1  bm25 rm3   94817                    2004.cikm_conference-2004.6   \n",
       "2   1  bm25 rm3  110064                   2012.trec_conference-2012.15   \n",
       "3   1  bm25 rm3  101702                  2018.ictir_conference-2018.36   \n",
       "4   1  bm25 rm3  102200               2012.wwwconf_conference-2012c.74   \n",
       "5   1  bm25 rm3  126534  2017.tois_journal-ir0anthology0volumeA36A2.11   \n",
       "6   1  bm25 rm3   53598                                       P13-2109   \n",
       "7   1  bm25 rm3   94788                  2009.cikm_conference-2009.315   \n",
       "8   1  bm25 rm3   41184                                       W19-4331   \n",
       "9   1  bm25 rm3  110712                   2006.trec_conference-2006.51   \n",
       "\n",
       "        score  rank  \\\n",
       "0  709.886841     0   \n",
       "1  708.151855     1   \n",
       "2  708.033813     2   \n",
       "3  708.010620     3   \n",
       "4  707.888000     4   \n",
       "5  707.834106     5   \n",
       "6  707.796265     6   \n",
       "7  707.705261     7   \n",
       "8  707.695557     8   \n",
       "9  707.596924     9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \n",
       "0  When documents are very long, BM25 fails!\\n\\n\\n ABSTRACTWe reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which \"shifts\" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]: Retrieval models\\nGeneral TermsAlgorithms Keywords BM25, BM25L, term frequency, very long documents\\nMOTIVATIONThe Okapi BM25 retrieval function  has been the state-of-the-art for nearly two decades. BM25 scores a document D with respect to query Q as follows:where c(q, Q) is the count of q in Q, N is the total number of documents, df (q) is the document frequency of q, and k3 is a parameter. Following [1], we use a modified IDF formula in BM25 to avoid its problem of possibly negative IDF values. A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula:where |D| represents document length, avdl stands for average document length, c(q, D) is the raw TF of q in D, and b and k1 are two parameters. c (q, D) is the normalized TF by document length using pivoted length normalization .Copyright is held by the author/owner(s). SIGIR'11, July 24-28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07. \\nBOOSTING VERY LONG DOCUMENTSIn order to avoid overly-penalizing very long documents, we need to add a constraint in TF normalization to make sure that the \"score gap\" of f One heuristic way to achieve this goal is to define f (q, D) as follows:  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Simple BM25 extension to multiple weighted fields\\n\\n\\n ABSTRACTThis paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the nonlinear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        DCU@TRECMed 2012: Using adhoc Baselines for Domain-Specific Retrieval\\n\\n\\n This paper describes the first participation of DCU in the TREC Medical Records Track (TRECMed) 2012. We performed initial experiments on the the 2011 TRECMed data based on the BM25 retrieval model. Surprisingly, we found that the standard BM25 model with default parameters performs comparable to the best automatic runs submitted to TRECMed 2011 and our experiments would have ranked among the top four out of 29 participating groups. We expected that some form of domain adaptation would increase performance. However, results on the 2011 data proved otherwise: query expansion decreased performance, and filtering and reranking by term proximity also decreased performance slightly. We submitted four runs based on the BM25 retrieval model to TRECMed 2012 using standard BM25, standard query expansion, result filtering, and concept-based query expansion. Official results for 2012 confirm that domain-specific knowledge, as applied by us, does not increase performance compared to the BM25 baseline.  \n",
       "3                                                                                                                                                    StatBM25: An Aggregative and Statistical Approach for Document Ranking\\n\\n\\n ABSTRACTIn Information Retrieval and Web Search, BM25 is one of the most influential probabilistic retrieval formulas for document weighting and ranking. BM25 involves three parameters k 1 , k 3 and b, which provide scalar approximation and scaling of important document features such as term frequency, document frequency, and document length. We investigate in this paper aggregative and statistical document features for document ranking. Shortly speaking, a statistically adjusted BM25 is used to score in an aggregative way on virtual documents, which are generated by randomly combining documents from the original collection. The problem size, in the number of virtual documents to be ranked, is an expansion to the problem size of the original problem. As a result, ranking is actually realized through performing statistical sampling. Rejection Sampling, a simple Monte Carlo sampling method is used at present. This new framework is called StatBM25, in emphasizing first the fact that the original problem domain space is K-expanded (a concept to be further explained in the paper); Further, statistical sampling is employed in the model. Empirical studies are performed on several standard test collections, where StatBM25 demonstrates convincingly high degree of both uniqueness and effectiveness compared to BM25. This means, in our belief, that StatBM25 as a statistically smoothed and normalized variant to BM25, might eventually lead to discoveries of useful new statistic measures for document ranking.  \n",
       "4                                                                                                                                                                                                                                H2RDF: adaptive query processing on RDF data in the cloud\\n\\n\\n ABSTRACTIn this work we present H2RDF , a fully distributed RDF store that combines the MapReduce processing framework with a NoSQL distributed data store. Our system features two unique characteristics that enable efficient processing of both simple and multi-join SPARQL queries on virtually unlimited number of triples: Join algorithms that execute joins according to query selectivity to reduce processing; and adaptive choice among centralized and distributed (MapReduce-based) join execution for fast query responses. Our system efficiently answers both simple joins and complex multivariate queries and easily scales to 3 billion triples using a small cluster of 9 worker nodes. H2RDF outperforms state-of-the-art distributed solutions in multi-join and nonselective queries while achieving comparable performance to centralized solutions in selective queries. In this demonstration we showcase the system's functionality through an interactive GUI. Users will be able to execute predefined or custom-made SPARQL queries on datasets of different sizes, using different join algorithms. Moreover, they can repeat all queries utilizing a different number of cluster resources. Using real-time cluster monitoring and detailed statistics, participants will be able to understand the advantages of different execution schemes versus the input data as well as the scalability properties of H2RDF over both the data size and the available worker resources.  \n",
       "5                                                                                                                                                                     Local Representative-Based Matrix Factorization for Cold-Start Recommendation\\n\\n\\n Cold-start recommendation is one of the most challenging problems in recommender systems. An important approach to cold-start recommendation is to conduct an interview for new users, called the interview-based approach. Among the interview-based methods, Representative-Based Matrix Factorization (RBMF)  provides an effective solution with appealing merits: it represents users over selected representative items, which makes the recommendations highly intuitive and interpretable. However, RBMF only utilizes a global set of representative items to model all users. Such a representation is somehow too strict and may not be flexible enough to capture varying users' interests. To address this problem, we propose a novel interview-based model to dynamically create meaningful user groups using decision trees and then select local representative items for different groups. A two-round interview is performed for a new user. In the first round, l 1 global questions are issued for group division, while in the second round, l 2 local-group-specific questions are given to derive local representation. We collect the feedback on the (l 1 + l 2 ) items to learn the user representations. By putting these steps together, we develop a joint optimization model, named local representative-based matrix factorization, for new user recommendations. Extensive experiments on three public datasets have demonstrated the effectiveness of the proposed model compared with several competitive baselines.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers\\n\\n\\n We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD 3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German).  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HDDBrs middleware for implementing highly available distributed databases\\n\\n\\n ABSTRACTOur demo presents HDDB RS , a middle tier offering to clients a highly available distributed database interface using Reed Solomon codes to compute parity data. Parity data is stored in dedicated parity DB backends, is synchronously updated and allows recovering from multiple DB backend unavailability. HDDB RS middle tier is implemented in JAVA using standard technology, and is designed to be interoperable with any database engine that provides a JDBC driver and implements X/open XA protocol.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Modality-based Factorization for Multimodal Fusion\\n\\n\\n We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M + 1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  MG4J at TREC 2006\\n\\n\\n MG4J participated in the ad hoc task of the Terabyte Track (find all the relevant documents with high precision from 25.2 million pages from the .gov domain) at TREC 2006. It was the second time the MG4J group participated to TREC. For this year, we integrated standard techniques (such as stemming and BM25 scoring) into MG4J, and submitted also automatic runs based on trivial query expansion techniques.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ance.search('bm25 rm3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 1 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n",
      "***** faiss search for 1 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.02shard/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>103415</td>\n",
       "      <td>2015.wwwconf_conference-2015.10</td>\n",
       "      <td>710.254028</td>\n",
       "      <td>0</td>\n",
       "      <td>Essential Web Pages Are Easy to Find\\n\\n\\n ABSTRACTIn this paper we address the problem of estimating the index size needed by web search engines to answer as many queries as possible by exploiting the marked difference between query and click frequencies. We provide a possible formal definition for the notion of essential web pages as those that cover a large fraction of distinct queries -i.e., we look at the problem as a version of MAXCOVER. Although in general MAXCOVER is approximable to within a factor of 1 − 1/e ≈ 0.632 from the optimum, we provide a condition under which the greedy algorithm does find the actual best cover (or remains at a known bounded factor from it). The extra check for optimality (or for bounding the ratio from the optimum) comes at a negligible algorithmic cost. Moreover, in most practical instances of this problem, the algorithm is able to provide solutions that are provably optimal, or close to optimal. We relate this observed phenomenon to some properties of the queries' click graph. Our experimental results confirm that a small number of web pages can respond to a large fraction of the queries (e.g., 0.4% of the pages answers 20% of the queries). Our approach can be used in several related search applications, and has in fact an even more general appeal -as a first example, our preliminary experimental study confirms that our algorithm has extremely good performances on other (social network based) MAXCOVER instances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>90593</td>\n",
       "      <td>2005.airs_conference-2005.48</td>\n",
       "      <td>710.203308</td>\n",
       "      <td>1</td>\n",
       "      <td>Calculating Webpage Importance with Site Structure Constraints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>84119</td>\n",
       "      <td>2003.sigirconf_conference-2003.61</td>\n",
       "      <td>709.946533</td>\n",
       "      <td>2</td>\n",
       "      <td>Searchers' criteria For assessing web pages\\n\\n\\n ABSTRACTWe investigate the criteria used by online searchers when assessing the relevance of web pages to information-seeking tasks. Twenty four searchers were given three tasks each, and indicated the features of web pages which they employed when deciding about the usefulness of the pages. These tasks were presented within the context of a simulated work-task situation. The results of this study provide a set of criteria used by searchers to decide about the utility of web pages. Such criteria have implications for the design of systems that use or recommend web pages, as well as to authors of web pages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>97057</td>\n",
       "      <td>2013.cikm_conference-2013.350</td>\n",
       "      <td>709.746582</td>\n",
       "      <td>3</td>\n",
       "      <td>READFAST: high-relevance search-engine for big text\\n\\n\\n ABSTRACTRelevance of search-results is a key factor for any search engine. In order to return and rank the Web-pages that are most relevant to the query, contemporary search engines use complex ranking functions that depend on hundreds of features. For example, presence or absence of the query keywords on the page, their proximity, frequencies, HTML markup are just a few to name. Additional features might include fonts, tags, hyperlinks, metadata, and parts of the Web-page description. All this information is used by the search-engine to rank HTML Web pages returned to the user, but is unfortunately absent in free text that has no HTML markup, tags, hyperlinks, and any other metadata, except implicit natural language structure.Here we demonstrate one of the first Big text search engines that leverages hidden structure of the natural language sentences in order to process user queries and return more relevant search-results than a standard keyword-search. It provides a structured index extracted from the text using Natural Language Processing (NLP) that can be used to browse and query free text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>80596</td>\n",
       "      <td>2014.sigirconf_conference-2014.150</td>\n",
       "      <td>709.584961</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyzing the content emphasis of web search engines\\n\\n\\n ABSTRACTMillions of people search the Web each day. As a consequence, the ranking algorithms employed by Web search engines have a profound influence on which pages users visit. Characterizing this influence, and informing users when different engines favor certain sites or points of view, enables more transparent access to the Web's information.We present PAWS, a platform for analyzing differences among Web search engines. PAWS measures content emphasis: the degree to which differences across search engines' rankings correlate with features of the ranked content, including point of view (e.g., positive or negative orientation toward their company's products) and advertisements. We propose an approach for identifying the orientations in search results at scale, through a novel technique that minimizes the expected number of human judgments required. We apply PAWS to news search on Google and Bing, and find no evidence that the engines emphasize results that express positive orientation toward the engine company's products. We do find that the engines emphasize particular news sites, and that they also favor pages containing their company's advertisements, as opposed to competitor advertisements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>80794</td>\n",
       "      <td>2008.sigirconf_conference-2008.59</td>\n",
       "      <td>709.374512</td>\n",
       "      <td>5</td>\n",
       "      <td>BrowseRank: letting web users vote for page importance\\n\\n\\n ABSTRACTThis paper proposes a new method for computing page importance, referred to as BrowseRank. The conventional approach to compute page importance is to exploit the link graph of the web and to build a model based on that graph. For instance, PageRank is such an algorithm, which employs a discrete-time Markov process as the model. Unfortunately, the link graph might be incomplete and inaccurate with respect to data for determining page importance, because links can be easily added and deleted by web content creators. In this paper, we propose computing page importance by using a 'user browsing graph' created from user behavior data. In this graph, vertices represent pages and directed edges represent transitions between pages in the users' web browsing history. Furthermore, the lengths of staying time spent on the pages by users are also included. The user browsing graph is more reliable than the link graph for inferring page importance. This paper further proposes using the continuous-time Markov process on the user browsing graph as a model and computing the stationary probability distribution of the process as page importance. An efficient algorithm for this computation has also been devised. In this way, we can leverage hundreds of millions of users' implicit voting on page importance. Experimental results show that BrowseRank indeed outperforms the baseline methods such as PageRank and TrustRank in several tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>123811</td>\n",
       "      <td>2019.ipm_journal-ir0anthology0volumeA56A3.40</td>\n",
       "      <td>709.356201</td>\n",
       "      <td>6</td>\n",
       "      <td>An efficient page ranking approach based on vector norms using sNorm(p) algorithm\\n\\n\\n In the whole world, the internet is exercised by millions of people every day for information retrieval. Even for a small to smaller task like fixing a fan, to cook food or even to iron clothes persons opt to search the web. To fulfill the information needs of people, there are billions of web pages, each having a different degree of relevance to the topic of interest (TOI), scattered throughout the web but this huge size makes manual information retrieval impossible. The page ranking algorithm is an integral part of search engines as it arranges web pages associated with a queried TOI in order of their relevance level. It, therefore, plays an important role in regulating the search quality and user experience for information retrieval. PageRank, HITS, and SALSA are well-known page ranking algorithm based on link structure analysis of a seed set, but ranking given by them has not yet been efficient. In this paper, we propose a variant of SALSA to give sNorm(p) for the efficient ranking of web pages. Our approach relies on a p-Norm from Vector Norm family in a novel way for the ranking of web pages as Vector Norms can reduce the impact of low authority weight in hub weight calculation in an efficient way. Our study, then compares the rankings given by PageRank, HITS, SALSA, and sNorm(p) to the same pages in the same query. The effectiveness of the proposed approach over state of the art methods has been shown using performance measurement technique, Mean Reciprocal Rank (MRR), Precision, Mean Average Precision (MAP), Discounted Cumulative Gain (DCG) and Normalized DCG (NDCG). The experimentation is performed on a dataset acquired after pre-processing of the results collected from initial few pages retrieved for a query by the Google search engine. Based on the type and amount of in-hand domain expertise 30 queries are designed. The extensive evaluation and result analysis are performed using MRR, Precision@k, MAP, DCG, and NDCG as the performance measuring statistical metrics. Furthermore, results are statistically verified using a significance test. Findings show that our approach outperforms state of the art methods by attaining 0.8666 as MRR value, 0.7957 as MAP value. Thus contributing to the improvement in the ranking of web pages more efficiently as compared to its counterparts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>114536</td>\n",
       "      <td>2013.tweb_journal-ir0anthology0volumeA7A1.0</td>\n",
       "      <td>709.356140</td>\n",
       "      <td>7</td>\n",
       "      <td>Measuring the Visual Complexities of Web Pages\\n\\n\\n Visual complexities (VisComs) of Web pages significantly affect user experience, and automatic evaluation can facilitate a large number of Web-based applications. The construction of a model for measuring the VisComs of Web pages requires the extraction of typical features and learning based on labeled Web pages. However, as far as the authors are aware, little headway has been made on measuring VisCom in Web mining and machine learning. The present article provides a new approach combining Web mining techniques and machine learning algorithms for measuring the VisComs of Web pages. The structure of a Web page is first analyzed, and the layout is then extracted. Using a Web page as a semistructured image, three classes of features are extracted to construct a feature vector. The feature vector is fed into a learned measuring function to calculate the VisCom of the page.In the proposed approach of the present study, the type of the measuring function and its learning depend on the quantification strategy for VisCom. Aside from using a category and a score to represent VisCom as existing work, this study presents a new strategy utilizing a distribution to quantify the VisCom of a Web page. Empirical evaluation suggests the effectiveness of the proposed approach in terms of both features and learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>97037</td>\n",
       "      <td>2013.cikm_conference-2013.330</td>\n",
       "      <td>709.293213</td>\n",
       "      <td>8</td>\n",
       "      <td>Incorporating the surfing behavior of web users into pagerank\\n\\n\\n ABSTRACTIn large-scale commercial web search engines, estimating the importance of a web page is a crucial ingredient in ranking web search results. So far, to assess the importance of web pages, two different types of feedback have been taken into account, independent of each other: the feedback obtained from the hyperlink structure among the web pages (e.g., PageRank) or the web browsing patterns of users (e.g., BrowseRank). Unfortunately, both types of feedback have certain drawbacks. While the former lacks the user preferences and is vulnerable to malicious intent, the latter suffers from sparsity and hence low web coverage. In this work, we combine these two types of feedback under a hybrid page ranking model in order to alleviate the above-mentioned drawbacks. Our empirical results indicate that the proposed model leads to better estimation of page importance according to an evaluation metric that relies on user click feedback obtained from web search query logs. We conduct all of our experiments in a realistic setting, using a very large scale web page collection (around 6.5 billion web pages) and web browsing data (around two billion web page visits).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>measure importance of web pages</td>\n",
       "      <td>108369</td>\n",
       "      <td>2001.wwwconf_conference-2001p.53</td>\n",
       "      <td>709.262085</td>\n",
       "      <td>9</td>\n",
       "      <td>Keeping Web Indices up-to-date\\n\\n\\n Search engines play a crucial role in the Web. Without search engines large parts of the Web becomes inaccessible for the majority of users. Search engines can make new and smaller sites accessible at low cost. Without them, other media, such as Television, would be needed to advertise the existence new site on the Web, only large commercial sites can follow this path. The Web would be endangered to become dominated by a few, well known sites. A crucial problem of search engines is to keep their index up-to-date. Especially if the index grows, the effort needed to update the index increases, since Web documents are dynamic and thus already stored data becomes obsolete. There have been various attempts to monitor the evolvement of the Web [1][7]. However, we believe, that change model used in prior work overestimates the rate of change due to an inadequate change model. Our change model has been adapted from the information retrieval field to distinguish index relevant changes from irrelevant modifications in Web documents, e.g. simple spelling corrections or dynamic advertisement links. We have monitored multiple smaller collections of documents over a time period of six month to measure the documents change.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                            query   docid  \\\n",
       "0   1  measure importance of web pages  103415   \n",
       "1   1  measure importance of web pages   90593   \n",
       "2   1  measure importance of web pages   84119   \n",
       "3   1  measure importance of web pages   97057   \n",
       "4   1  measure importance of web pages   80596   \n",
       "5   1  measure importance of web pages   80794   \n",
       "6   1  measure importance of web pages  123811   \n",
       "7   1  measure importance of web pages  114536   \n",
       "8   1  measure importance of web pages   97037   \n",
       "9   1  measure importance of web pages  108369   \n",
       "\n",
       "                                          docno       score  rank  \\\n",
       "0               2015.wwwconf_conference-2015.10  710.254028     0   \n",
       "1                  2005.airs_conference-2005.48  710.203308     1   \n",
       "2             2003.sigirconf_conference-2003.61  709.946533     2   \n",
       "3                 2013.cikm_conference-2013.350  709.746582     3   \n",
       "4            2014.sigirconf_conference-2014.150  709.584961     4   \n",
       "5             2008.sigirconf_conference-2008.59  709.374512     5   \n",
       "6  2019.ipm_journal-ir0anthology0volumeA56A3.40  709.356201     6   \n",
       "7   2013.tweb_journal-ir0anthology0volumeA7A1.0  709.356140     7   \n",
       "8                 2013.cikm_conference-2013.330  709.293213     8   \n",
       "9              2001.wwwconf_conference-2001p.53  709.262085     9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Essential Web Pages Are Easy to Find\\n\\n\\n ABSTRACTIn this paper we address the problem of estimating the index size needed by web search engines to answer as many queries as possible by exploiting the marked difference between query and click frequencies. We provide a possible formal definition for the notion of essential web pages as those that cover a large fraction of distinct queries -i.e., we look at the problem as a version of MAXCOVER. Although in general MAXCOVER is approximable to within a factor of 1 − 1/e ≈ 0.632 from the optimum, we provide a condition under which the greedy algorithm does find the actual best cover (or remains at a known bounded factor from it). The extra check for optimality (or for bounding the ratio from the optimum) comes at a negligible algorithmic cost. Moreover, in most practical instances of this problem, the algorithm is able to provide solutions that are provably optimal, or close to optimal. We relate this observed phenomenon to some properties of the queries' click graph. Our experimental results confirm that a small number of web pages can respond to a large fraction of the queries (e.g., 0.4% of the pages answers 20% of the queries). Our approach can be used in several related search applications, and has in fact an even more general appeal -as a first example, our preliminary experimental study confirms that our algorithm has extremely good performances on other (social network based) MAXCOVER instances.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Calculating Webpage Importance with Site Structure Constraints  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Searchers' criteria For assessing web pages\\n\\n\\n ABSTRACTWe investigate the criteria used by online searchers when assessing the relevance of web pages to information-seeking tasks. Twenty four searchers were given three tasks each, and indicated the features of web pages which they employed when deciding about the usefulness of the pages. These tasks were presented within the context of a simulated work-task situation. The results of this study provide a set of criteria used by searchers to decide about the utility of web pages. Such criteria have implications for the design of systems that use or recommend web pages, as well as to authors of web pages.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             READFAST: high-relevance search-engine for big text\\n\\n\\n ABSTRACTRelevance of search-results is a key factor for any search engine. In order to return and rank the Web-pages that are most relevant to the query, contemporary search engines use complex ranking functions that depend on hundreds of features. For example, presence or absence of the query keywords on the page, their proximity, frequencies, HTML markup are just a few to name. Additional features might include fonts, tags, hyperlinks, metadata, and parts of the Web-page description. All this information is used by the search-engine to rank HTML Web pages returned to the user, but is unfortunately absent in free text that has no HTML markup, tags, hyperlinks, and any other metadata, except implicit natural language structure.Here we demonstrate one of the first Big text search engines that leverages hidden structure of the natural language sentences in order to process user queries and return more relevant search-results than a standard keyword-search. It provides a structured index extracted from the text using Natural Language Processing (NLP) that can be used to browse and query free text.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Analyzing the content emphasis of web search engines\\n\\n\\n ABSTRACTMillions of people search the Web each day. As a consequence, the ranking algorithms employed by Web search engines have a profound influence on which pages users visit. Characterizing this influence, and informing users when different engines favor certain sites or points of view, enables more transparent access to the Web's information.We present PAWS, a platform for analyzing differences among Web search engines. PAWS measures content emphasis: the degree to which differences across search engines' rankings correlate with features of the ranked content, including point of view (e.g., positive or negative orientation toward their company's products) and advertisements. We propose an approach for identifying the orientations in search results at scale, through a novel technique that minimizes the expected number of human judgments required. We apply PAWS to news search on Google and Bing, and find no evidence that the engines emphasize results that express positive orientation toward the engine company's products. We do find that the engines emphasize particular news sites, and that they also favor pages containing their company's advertisements, as opposed to competitor advertisements.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           BrowseRank: letting web users vote for page importance\\n\\n\\n ABSTRACTThis paper proposes a new method for computing page importance, referred to as BrowseRank. The conventional approach to compute page importance is to exploit the link graph of the web and to build a model based on that graph. For instance, PageRank is such an algorithm, which employs a discrete-time Markov process as the model. Unfortunately, the link graph might be incomplete and inaccurate with respect to data for determining page importance, because links can be easily added and deleted by web content creators. In this paper, we propose computing page importance by using a 'user browsing graph' created from user behavior data. In this graph, vertices represent pages and directed edges represent transitions between pages in the users' web browsing history. Furthermore, the lengths of staying time spent on the pages by users are also included. The user browsing graph is more reliable than the link graph for inferring page importance. This paper further proposes using the continuous-time Markov process on the user browsing graph as a model and computing the stationary probability distribution of the process as page importance. An efficient algorithm for this computation has also been devised. In this way, we can leverage hundreds of millions of users' implicit voting on page importance. Experimental results show that BrowseRank indeed outperforms the baseline methods such as PageRank and TrustRank in several tasks.  \n",
       "6  An efficient page ranking approach based on vector norms using sNorm(p) algorithm\\n\\n\\n In the whole world, the internet is exercised by millions of people every day for information retrieval. Even for a small to smaller task like fixing a fan, to cook food or even to iron clothes persons opt to search the web. To fulfill the information needs of people, there are billions of web pages, each having a different degree of relevance to the topic of interest (TOI), scattered throughout the web but this huge size makes manual information retrieval impossible. The page ranking algorithm is an integral part of search engines as it arranges web pages associated with a queried TOI in order of their relevance level. It, therefore, plays an important role in regulating the search quality and user experience for information retrieval. PageRank, HITS, and SALSA are well-known page ranking algorithm based on link structure analysis of a seed set, but ranking given by them has not yet been efficient. In this paper, we propose a variant of SALSA to give sNorm(p) for the efficient ranking of web pages. Our approach relies on a p-Norm from Vector Norm family in a novel way for the ranking of web pages as Vector Norms can reduce the impact of low authority weight in hub weight calculation in an efficient way. Our study, then compares the rankings given by PageRank, HITS, SALSA, and sNorm(p) to the same pages in the same query. The effectiveness of the proposed approach over state of the art methods has been shown using performance measurement technique, Mean Reciprocal Rank (MRR), Precision, Mean Average Precision (MAP), Discounted Cumulative Gain (DCG) and Normalized DCG (NDCG). The experimentation is performed on a dataset acquired after pre-processing of the results collected from initial few pages retrieved for a query by the Google search engine. Based on the type and amount of in-hand domain expertise 30 queries are designed. The extensive evaluation and result analysis are performed using MRR, Precision@k, MAP, DCG, and NDCG as the performance measuring statistical metrics. Furthermore, results are statistically verified using a significance test. Findings show that our approach outperforms state of the art methods by attaining 0.8666 as MRR value, 0.7957 as MAP value. Thus contributing to the improvement in the ranking of web pages more efficiently as compared to its counterparts.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Measuring the Visual Complexities of Web Pages\\n\\n\\n Visual complexities (VisComs) of Web pages significantly affect user experience, and automatic evaluation can facilitate a large number of Web-based applications. The construction of a model for measuring the VisComs of Web pages requires the extraction of typical features and learning based on labeled Web pages. However, as far as the authors are aware, little headway has been made on measuring VisCom in Web mining and machine learning. The present article provides a new approach combining Web mining techniques and machine learning algorithms for measuring the VisComs of Web pages. The structure of a Web page is first analyzed, and the layout is then extracted. Using a Web page as a semistructured image, three classes of features are extracted to construct a feature vector. The feature vector is fed into a learned measuring function to calculate the VisCom of the page.In the proposed approach of the present study, the type of the measuring function and its learning depend on the quantification strategy for VisCom. Aside from using a category and a score to represent VisCom as existing work, this study presents a new strategy utilizing a distribution to quantify the VisCom of a Web page. Empirical evaluation suggests the effectiveness of the proposed approach in terms of both features and learning algorithms.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Incorporating the surfing behavior of web users into pagerank\\n\\n\\n ABSTRACTIn large-scale commercial web search engines, estimating the importance of a web page is a crucial ingredient in ranking web search results. So far, to assess the importance of web pages, two different types of feedback have been taken into account, independent of each other: the feedback obtained from the hyperlink structure among the web pages (e.g., PageRank) or the web browsing patterns of users (e.g., BrowseRank). Unfortunately, both types of feedback have certain drawbacks. While the former lacks the user preferences and is vulnerable to malicious intent, the latter suffers from sparsity and hence low web coverage. In this work, we combine these two types of feedback under a hybrid page ranking model in order to alleviate the above-mentioned drawbacks. Our empirical results indicate that the proposed model leads to better estimation of page importance according to an evaluation metric that relies on user click feedback obtained from web search query logs. We conduct all of our experiments in a realistic setting, using a very large scale web page collection (around 6.5 billion web pages) and web browsing data (around two billion web page visits).  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Keeping Web Indices up-to-date\\n\\n\\n Search engines play a crucial role in the Web. Without search engines large parts of the Web becomes inaccessible for the majority of users. Search engines can make new and smaller sites accessible at low cost. Without them, other media, such as Television, would be needed to advertise the existence new site on the Web, only large commercial sites can follow this path. The Web would be endangered to become dominated by a few, well known sites. A crucial problem of search engines is to keep their index up-to-date. Especially if the index grows, the effort needed to update the index increases, since Web documents are dynamic and thus already stored data becomes obsolete. There have been various attempts to monitor the evolvement of the Web [1][7]. However, we believe, that change model used in prior work overestimates the rate of change due to an inadequate change model. Our change model has been adapted from the information retrieval field to distinguish index relevant changes from irrelevant modifications in Web documents, e.g. simple spelling corrections or dynamic advertisement links. We have monitored multiple smaller collections of documents over a time period of six month to measure the documents change.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ance.search('measure importance of web pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 1 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n",
      "***** faiss search for 1 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.89shard/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>118784</td>\n",
       "      <td>2005.jasis_journal-ir0anthology0volumeA56A1.7</td>\n",
       "      <td>713.220825</td>\n",
       "      <td>0</td>\n",
       "      <td>PageRank and Interaction Information Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>108211</td>\n",
       "      <td>2003.wwwconf_conference-2003p.170</td>\n",
       "      <td>712.280334</td>\n",
       "      <td>1</td>\n",
       "      <td>Usage Aware PageRank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>108646</td>\n",
       "      <td>2005.wwwconf_conference-2005si.74</td>\n",
       "      <td>711.932007</td>\n",
       "      <td>2</td>\n",
       "      <td>TruRank: taking PageRank to the limit\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain depending on a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. It is common belief that values of α closer to 1 give a \"truer to the web\" PageRank, but a small α accelerates convergence. Recently, however, it has been shown that when α = 1 all pages in the core component are very likely to have rank 0 [1]. This behaviour makes it difficult to understand PageRank when α ≈ 1, as it converges to a meaningless value for most pages. We propose a simple and natural modification to the standard preprocessing performed on the adjacency matrix of the graph, resulting in a ranking scheme we call TruRank. TruRank ranks the web with principles almost identical to PageRank, but it gives meaningful values also when α ≈ 1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>110287</td>\n",
       "      <td>2008.trec_conference-2008.58</td>\n",
       "      <td>711.626587</td>\n",
       "      <td>3</td>\n",
       "      <td>Weighted PageRank: Cluster-Related Weights\\n\\n\\n PageRank is a way to rank Web pages taking into account hyper-link structure of the Web. PageRank provides efficient and simple method to find out ranking of Web pages exploiting hyper-link structure of the Web. However, it produces just an approximation of the ranking since the random surfer model uses just uniform distributions for all situation of choice happening during the surf process. In particular, this implies that the random surfer has no preferences. The assumption is limited by its nature. Personalized PageRank was designed to solve the problem but it is still quite restrictive since it assumes non-uniform preferences just at jumping to arbitrary page on the Web and non-preferring behaviour when following outgoing hyper-links. Taking into account these limitations and restrictions of PageRank and Personalized PageRank we propose Weighted PageRank where we are free to weight hyper-links according any possible preferring behaviour of a user. In particular, cluster-related weights are considered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>108551</td>\n",
       "      <td>2005.wwwconf_conference-2005.62</td>\n",
       "      <td>711.200256</td>\n",
       "      <td>4</td>\n",
       "      <td>PageRank as a function of the damping factor\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain. The chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. Recently, however, the behaviour of PageRank with respect to changes in α was discovered to be useful in link-spam detection . Moreover, an analytical justification of the value chosen for α is still missing. In this paper, we give the first mathematical analysis of PageRank when α changes. In particular, we show that, contrarily to popular belief, for real-world graphs values of α close to 1 do not give a more meaningful ranking. Then, we give closed-form formulae for PageRank derivatives of any order, and an extension of the Power Method that approximates them with convergence O t k α t for the k-th derivative. Finally, we show a tight connection between iterated computation and analytical behaviour by proving that the k-th iteration of the Power Method gives exactly the PageRank value obtained using a Maclaurin polynomial of degree k. The latter result paves the way towards the application of analytical methods to the study of PageRank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>104348</td>\n",
       "      <td>2021.wwwconf_conference-2021.335</td>\n",
       "      <td>710.360779</td>\n",
       "      <td>5</td>\n",
       "      <td>Fairness-Aware PageRank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>91623</td>\n",
       "      <td>2007.ecir_conference-2007.54</td>\n",
       "      <td>710.310425</td>\n",
       "      <td>6</td>\n",
       "      <td>PageRank: When Order Changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>126911</td>\n",
       "      <td>2009.tois_journal-ir0anthology0volumeA27A4.0</td>\n",
       "      <td>710.207275</td>\n",
       "      <td>7</td>\n",
       "      <td>PageRank: Functional dependencies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>108607</td>\n",
       "      <td>2005.wwwconf_conference-2005si.35</td>\n",
       "      <td>710.118713</td>\n",
       "      <td>8</td>\n",
       "      <td>TotalRank: ranking without damping\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain obtained by perturbing the transition matrix of a web graph with a damping factor α that spreads part of the rank. The choice of α is eminently empirical, but most applications use α = 0.85; nonetheless, the selection of α is critical, and some believe that link farms may use this choice adversarially. Recent results  prove that the PageRank of a page is a rational function of α, and that this function can be approximated quite efficiently: this fact can be used to define a new form of ranking, TotalRank, that averages PageRanks over all possible α's. We show how this rank can be computed efficiently, and provide some preliminary experimental results on its quality and comparisons with PageRank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>PageRank</td>\n",
       "      <td>116434</td>\n",
       "      <td>2012.jasis_journal-ir0anthology0volumeA63A8.7</td>\n",
       "      <td>710.070374</td>\n",
       "      <td>9</td>\n",
       "      <td>Web query disambiguation using PageRank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid     query   docid                                          docno  \\\n",
       "0   1  PageRank  118784  2005.jasis_journal-ir0anthology0volumeA56A1.7   \n",
       "1   1  PageRank  108211              2003.wwwconf_conference-2003p.170   \n",
       "2   1  PageRank  108646              2005.wwwconf_conference-2005si.74   \n",
       "3   1  PageRank  110287                   2008.trec_conference-2008.58   \n",
       "4   1  PageRank  108551                2005.wwwconf_conference-2005.62   \n",
       "5   1  PageRank  104348               2021.wwwconf_conference-2021.335   \n",
       "6   1  PageRank   91623                   2007.ecir_conference-2007.54   \n",
       "7   1  PageRank  126911   2009.tois_journal-ir0anthology0volumeA27A4.0   \n",
       "8   1  PageRank  108607              2005.wwwconf_conference-2005si.35   \n",
       "9   1  PageRank  116434  2012.jasis_journal-ir0anthology0volumeA63A8.7   \n",
       "\n",
       "        score  rank  \\\n",
       "0  713.220825     0   \n",
       "1  712.280334     1   \n",
       "2  711.932007     2   \n",
       "3  711.626587     3   \n",
       "4  711.200256     4   \n",
       "5  710.360779     5   \n",
       "6  710.310425     6   \n",
       "7  710.207275     7   \n",
       "8  710.118713     8   \n",
       "9  710.070374     9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    PageRank and Interaction Information Retrieval  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Usage Aware PageRank  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                              TruRank: taking PageRank to the limit\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain depending on a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. It is common belief that values of α closer to 1 give a \"truer to the web\" PageRank, but a small α accelerates convergence. Recently, however, it has been shown that when α = 1 all pages in the core component are very likely to have rank 0 [1]. This behaviour makes it difficult to understand PageRank when α ≈ 1, as it converges to a meaningless value for most pages. We propose a simple and natural modification to the standard preprocessing performed on the adjacency matrix of the graph, resulting in a ranking scheme we call TruRank. TruRank ranks the web with principles almost identical to PageRank, but it gives meaningful values also when α ≈ 1.  \n",
       "3                                                                                                                                                                                                                                                                                     Weighted PageRank: Cluster-Related Weights\\n\\n\\n PageRank is a way to rank Web pages taking into account hyper-link structure of the Web. PageRank provides efficient and simple method to find out ranking of Web pages exploiting hyper-link structure of the Web. However, it produces just an approximation of the ranking since the random surfer model uses just uniform distributions for all situation of choice happening during the surf process. In particular, this implies that the random surfer has no preferences. The assumption is limited by its nature. Personalized PageRank was designed to solve the problem but it is still quite restrictive since it assumes non-uniform preferences just at jumping to arbitrary page on the Web and non-preferring behaviour when following outgoing hyper-links. Taking into account these limitations and restrictions of PageRank and Personalized PageRank we propose Weighted PageRank where we are free to weight hyper-links according any possible preferring behaviour of a user. In particular, cluster-related weights are considered.  \n",
       "4  PageRank as a function of the damping factor\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain. The chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. Recently, however, the behaviour of PageRank with respect to changes in α was discovered to be useful in link-spam detection . Moreover, an analytical justification of the value chosen for α is still missing. In this paper, we give the first mathematical analysis of PageRank when α changes. In particular, we show that, contrarily to popular belief, for real-world graphs values of α close to 1 do not give a more meaningful ranking. Then, we give closed-form formulae for PageRank derivatives of any order, and an extension of the Power Method that approximates them with convergence O t k α t for the k-th derivative. Finally, we show a tight connection between iterated computation and analytical behaviour by proving that the k-th iteration of the Power Method gives exactly the PageRank value obtained using a Maclaurin polynomial of degree k. The latter result paves the way towards the application of analytical methods to the study of PageRank.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fairness-Aware PageRank  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      PageRank: When Order Changes  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 PageRank: Functional dependencies  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   TotalRank: ranking without damping\\n\\n\\n ABSTRACTPageRank is defined as the stationary state of a Markov chain obtained by perturbing the transition matrix of a web graph with a damping factor α that spreads part of the rank. The choice of α is eminently empirical, but most applications use α = 0.85; nonetheless, the selection of α is critical, and some believe that link farms may use this choice adversarially. Recent results  prove that the PageRank of a page is a rational function of α, and that this function can be approximated quite efficiently: this fact can be used to define a new form of ranking, TotalRank, that averages PageRanks over all possible α's. We show how this rank can be computed efficiently, and provide some preliminary experimental results on its quality and comparisons with PageRank.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Web query disambiguation using PageRank  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ance.search('PageRank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 1 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n",
      "***** faiss search for 1 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.59shard/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>97387</td>\n",
       "      <td>2016.cikm_conference-2016.133</td>\n",
       "      <td>708.554443</td>\n",
       "      <td>0</td>\n",
       "      <td>Q+Tree: An Efficient Quad Tree based Data Indexing for Parallelizing Dynamic and Reverse Skylines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>55531</td>\n",
       "      <td>2020.findings-emnlp.107</td>\n",
       "      <td>708.185791</td>\n",
       "      <td>1</td>\n",
       "      <td>{FQ}u{AD}: {F}rench Question Answering Dataset\\n\\n\\n Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In an effort to track the progress of French Question Answering models we propose a leaderboard and we have made the 1.0 version of our dataset freely available at https://illuin-tech. github.io/FQuAD-explorer/.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>22262</td>\n",
       "      <td>P06-1127</td>\n",
       "      <td>707.763428</td>\n",
       "      <td>2</td>\n",
       "      <td>Novel Association Measures Using Web Search with Double Checking\\n\\n\\n A web search with double checking model is proposed to explore the web as a live corpus. Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are presented. In the experiments on Rubenstein-Goodenough's benchmark data set, the CODC measure achieves correlation coefficient 0.8492, which competes with the performance (0.8914) of the model using WordNet. The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable. Further study on named entity clustering shows that the five measures are quite useful. In particular, CODC measure is very stable on wordword and name-name experiments. The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65% and 14.22% increase compared to the system without community expansion. All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>24565</td>\n",
       "      <td>L18-1544</td>\n",
       "      <td>707.695435</td>\n",
       "      <td>3</td>\n",
       "      <td>{T}-{RE}x: A Large Scale Alignment of Natural Language with Knowledge Base Triples\\n\\n\\n Alignments between natural language and Knowledge Base (KB) triples are an essential prerequisite for training machine learning approaches employed in a variety of Natural Language Processing problems. These include Relation Extraction, KB Population, Question Answering and Natural Language Generation from KB triples. Available datasets that provide those alignments are plagued by significant shortcomings -they are of limited size, they exhibit a restricted predicate coverage, and/or they are of unreported quality. To alleviate these shortcomings, we present T-REx, a dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences). T-REx is two orders of magnitude larger than the largest available alignments dataset and covers 2.5 times more predicates. Additionally, we stress the quality of this language resource thanks to an extensive crowdsourcing evaluation. T-REx is publicly available at https://w3id.org/t-rex.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>73950</td>\n",
       "      <td>2008.ntcir_workshop-2008.26</td>\n",
       "      <td>707.611755</td>\n",
       "      <td>4</td>\n",
       "      <td>ICT-Crossn: The System of Cross-lingual Information Retrieval of ICT in NTCIR-7\\n\\n\\n IR4QA is a new task in NTCIR-7, which intends to evaluate which IR techniques are more helpful to a QA system. This paper describes in detail the implementation of our IR4QA system, ICT-Crossn. The system consists of a query translation component that integrates the methods of phrase based statistical machine translation and OOV translation methods based on search engine, and a document retrieval component which combines outputs of multiple IR models with a linear model. We tune the parameters on the development set constructed on the dry run set. The official evaluation results show our method achieves a good performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>126862</td>\n",
       "      <td>2010.tois_journal-ir0anthology0volumeA29A1.0</td>\n",
       "      <td>707.588013</td>\n",
       "      <td>5</td>\n",
       "      <td>Efficient set intersection for inverted indexing\\n\\n\\n Conjunctive Boolean queries are a key component of modern information retrieval systems, especially when Webscale repositories are being searched. A conjunctive query q is equivalent to a |q|-way intersection over ordered sets of integers, where each set represents the documents containing one of the terms, and each integer in each set is an ordinal document identifier. As is the case with many computing applications, there is tension between the way in which the data is represented, and the ways in which it is to be manipulated. In particular, the sets representing index data for typical document collections are highly compressible, but are processed using random access techniques, meaning that methods for carrying out set intersections must be alert to issues to do with access patterns and data representation. Our purpose in this article is to explore these trade-offs, by investigating intersection techniques that make use of both uncompressed \"integer\" representations, as well as compressed arrangements. We also propose a simple hybrid method that provides both compact storage, and also faster intersection computations for conjunctive querying than is possible even with uncompressed representations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>96031</td>\n",
       "      <td>2007.cikm_conference-2007.104</td>\n",
       "      <td>707.563660</td>\n",
       "      <td>6</td>\n",
       "      <td>Ix-cubes: iceberg cubes for data warehousing and olap on xml data\\n\\n\\n ABSTRACTWith increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>22976</td>\n",
       "      <td>2021.eacl-main.106</td>\n",
       "      <td>707.371216</td>\n",
       "      <td>7</td>\n",
       "      <td>{NLQ}u{AD}: A Non-Factoid Long Question Answering Data Set\\n\\n\\n We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring documentlevel language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pretrained Transformer-based models, encouraging future research on long sequence language models. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>126636</td>\n",
       "      <td>2004.tois_journal-ir0anthology0volumeA22A2.4</td>\n",
       "      <td>707.353943</td>\n",
       "      <td>8</td>\n",
       "      <td>XIRQL: An XML query language based on information retrieval concepts\\n\\n\\n XIRQL (\"circle\") is an XML query language that incorporates imprecision and vagueness for both structural and content-oriented query conditions. The corresponding uncertainty is handled by a consistent probabilistic model. The core features of XIRQL are (1) document ranking based on index term weighting, (2) specificity-oriented search for retrieving the most relevant parts of documents, (3) datatypes with vague predicates for dealing with specific types of content and (4) structural vagueness for vague interpretation of structural query conditions. A XIRQL database may contain several classes of documents, where all documents in a class conform to the same DTD; links between documents also are supported. XIRQL queries are translated into a path algebra, which can be processed by our HyREX retrieval engine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>xquad</td>\n",
       "      <td>8122</td>\n",
       "      <td>W06-2701</td>\n",
       "      <td>707.348145</td>\n",
       "      <td>9</td>\n",
       "      <td>Representing and Querying Multi-dimensional Markup for Question Answering\\n\\n\\n This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efficiently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  query   docid                                         docno  \\\n",
       "0   1  xquad   97387                 2016.cikm_conference-2016.133   \n",
       "1   1  xquad   55531                       2020.findings-emnlp.107   \n",
       "2   1  xquad   22262                                      P06-1127   \n",
       "3   1  xquad   24565                                      L18-1544   \n",
       "4   1  xquad   73950                   2008.ntcir_workshop-2008.26   \n",
       "5   1  xquad  126862  2010.tois_journal-ir0anthology0volumeA29A1.0   \n",
       "6   1  xquad   96031                 2007.cikm_conference-2007.104   \n",
       "7   1  xquad   22976                            2021.eacl-main.106   \n",
       "8   1  xquad  126636  2004.tois_journal-ir0anthology0volumeA22A2.4   \n",
       "9   1  xquad    8122                                      W06-2701   \n",
       "\n",
       "        score  rank  \\\n",
       "0  708.554443     0   \n",
       "1  708.185791     1   \n",
       "2  707.763428     2   \n",
       "3  707.695435     3   \n",
       "4  707.611755     4   \n",
       "5  707.588013     5   \n",
       "6  707.563660     6   \n",
       "7  707.371216     7   \n",
       "8  707.353943     8   \n",
       "9  707.348145     9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Q+Tree: An Efficient Quad Tree based Data Indexing for Parallelizing Dynamic and Reverse Skylines  \n",
       "1                                                                                                                                                                                                                                                                         {FQ}u{AD}: {F}rench Question Answering Dataset\\n\\n\\n Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In an effort to track the progress of French Question Answering models we propose a leaderboard and we have made the 1.0 version of our dataset freely available at https://illuin-tech. github.io/FQuAD-explorer/.  \n",
       "2                                                                                     Novel Association Measures Using Web Search with Double Checking\\n\\n\\n A web search with double checking model is proposed to explore the web as a live corpus. Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are presented. In the experiments on Rubenstein-Goodenough's benchmark data set, the CODC measure achieves correlation coefficient 0.8492, which competes with the performance (0.8914) of the model using WordNet. The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable. Further study on named entity clustering shows that the five measures are quite useful. In particular, CODC measure is very stable on wordword and name-name experiments. The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65% and 14.22% increase compared to the system without community expansion. All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web.  \n",
       "3                                                                                                                                      {T}-{RE}x: A Large Scale Alignment of Natural Language with Knowledge Base Triples\\n\\n\\n Alignments between natural language and Knowledge Base (KB) triples are an essential prerequisite for training machine learning approaches employed in a variety of Natural Language Processing problems. These include Relation Extraction, KB Population, Question Answering and Natural Language Generation from KB triples. Available datasets that provide those alignments are plagued by significant shortcomings -they are of limited size, they exhibit a restricted predicate coverage, and/or they are of unreported quality. To alleviate these shortcomings, we present T-REx, a dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences). T-REx is two orders of magnitude larger than the largest available alignments dataset and covers 2.5 times more predicates. Additionally, we stress the quality of this language resource thanks to an extensive crowdsourcing evaluation. T-REx is publicly available at https://w3id.org/t-rex.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ICT-Crossn: The System of Cross-lingual Information Retrieval of ICT in NTCIR-7\\n\\n\\n IR4QA is a new task in NTCIR-7, which intends to evaluate which IR techniques are more helpful to a QA system. This paper describes in detail the implementation of our IR4QA system, ICT-Crossn. The system consists of a query translation component that integrates the methods of phrase based statistical machine translation and OOV translation methods based on search engine, and a document retrieval component which combines outputs of multiple IR models with a linear model. We tune the parameters on the development set constructed on the dry run set. The official evaluation results show our method achieves a good performance.  \n",
       "5  Efficient set intersection for inverted indexing\\n\\n\\n Conjunctive Boolean queries are a key component of modern information retrieval systems, especially when Webscale repositories are being searched. A conjunctive query q is equivalent to a |q|-way intersection over ordered sets of integers, where each set represents the documents containing one of the terms, and each integer in each set is an ordinal document identifier. As is the case with many computing applications, there is tension between the way in which the data is represented, and the ways in which it is to be manipulated. In particular, the sets representing index data for typical document collections are highly compressible, but are processed using random access techniques, meaning that methods for carrying out set intersections must be alert to issues to do with access patterns and data representation. Our purpose in this article is to explore these trade-offs, by investigating intersection techniques that make use of both uncompressed \"integer\" representations, as well as compressed arrangements. We also propose a simple hybrid method that provides both compact storage, and also faster intersection computations for conjunctive querying than is possible even with uncompressed representations.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Ix-cubes: iceberg cubes for data warehousing and olap on xml data\\n\\n\\n ABSTRACTWith increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.  \n",
       "7                                                                                                                                                                                                   {NLQ}u{AD}: A Non-Factoid Long Question Answering Data Set\\n\\n\\n We introduce NLQuAD, the first data set with baseline methods for non-factoid long question answering, a task requiring documentlevel language understanding. In contrast to existing span detection question answering data sets, NLQuAD has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union (IoU), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare BERT, RoBERTa, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. NLQuAD's samples exceed the input limitation of most pretrained Transformer-based models, encouraging future research on long sequence language models. 1  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                 XIRQL: An XML query language based on information retrieval concepts\\n\\n\\n XIRQL (\"circle\") is an XML query language that incorporates imprecision and vagueness for both structural and content-oriented query conditions. The corresponding uncertainty is handled by a consistent probabilistic model. The core features of XIRQL are (1) document ranking based on index term weighting, (2) specificity-oriented search for retrieving the most relevant parts of documents, (3) datatypes with vague predicates for dealing with specific types of content and (4) structural vagueness for vague interpretation of structural query conditions. A XIRQL database may contain several classes of documents, where all documents in a class conform to the same DTD; links between documents also are supported. XIRQL queries are translated into a path algebra, which can be processed by our HyREX retrieval engine.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Representing and Querying Multi-dimensional Markup for Question Answering\\n\\n\\n This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efficiently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ance.search('xquad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/terrierteam/pyterrier_ance.git\n",
    "!pip install faiss-cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
